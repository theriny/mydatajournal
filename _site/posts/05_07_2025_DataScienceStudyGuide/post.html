<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.29">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Therin Young">
<meta name="dcterms.date" content="2025-05-07">
<meta name="description" content="A place for all of the Data Science basics">

<title>My Data Science Study Guide – Journal</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0815c480559380816a4d1ea211a47e91.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-dfb324f25d9b1687192fa8be62ac8f9c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Journal</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">My Data Journal</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about/about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects/projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../scholarship/scholarship.html"> 
<span class="menu-text">Scholarship</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">My Data Science Study Guide</h1>
                  <div>
        <div class="description">
          A place for all of the Data Science basics
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">data science</div>
                <div class="quarto-category">interviewing</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Therin Young </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 7, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#data-science-study-guide" id="toc-data-science-study-guide" class="nav-link active" data-scroll-target="#data-science-study-guide">Data Science Study Guide</a>
  <ul class="collapse">
  <li><a href="#stochastic-gradient-descent-sgd" id="toc-stochastic-gradient-descent-sgd" class="nav-link" data-scroll-target="#stochastic-gradient-descent-sgd">1. Stochastic Gradient Descent (SGD)</a></li>
  <li><a href="#multicollinearity" id="toc-multicollinearity" class="nav-link" data-scroll-target="#multicollinearity">2. Multicollinearity</a></li>
  <li><a href="#common-machine-learning-algorithms" id="toc-common-machine-learning-algorithms" class="nav-link" data-scroll-target="#common-machine-learning-algorithms">3. Common Machine Learning Algorithms</a>
  <ul class="collapse">
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">1. Linear Regression</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">2. Logistic Regression</a></li>
  <li><a href="#decision-trees" id="toc-decision-trees" class="nav-link" data-scroll-target="#decision-trees">3. Decision Trees</a></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest">4. Random Forest</a></li>
  <li><a href="#support-vector-machines-svm" id="toc-support-vector-machines-svm" class="nav-link" data-scroll-target="#support-vector-machines-svm">5. Support Vector Machines (SVM)</a></li>
  <li><a href="#k-nearest-neighbors-knn" id="toc-k-nearest-neighbors-knn" class="nav-link" data-scroll-target="#k-nearest-neighbors-knn">6. K-Nearest Neighbors (KNN)</a></li>
  <li><a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link" data-scroll-target="#k-means-clustering">7. K-Means Clustering</a></li>
  <li><a href="#gradient-boosting-machines-gbm" id="toc-gradient-boosting-machines-gbm" class="nav-link" data-scroll-target="#gradient-boosting-machines-gbm">8. Gradient Boosting Machines (GBM)</a></li>
  </ul></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization">4. Regularization</a></li>
  <li><a href="#bias-variance-tradeoff" id="toc-bias-variance-tradeoff" class="nav-link" data-scroll-target="#bias-variance-tradeoff">5. Bias-Variance Tradeoff</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation">6. Cross-Validation</a></li>
  <li><a href="#dimensionality-reduction" id="toc-dimensionality-reduction" class="nav-link" data-scroll-target="#dimensionality-reduction">7. Dimensionality Reduction</a></li>
  <li><a href="#hyperparameter-tuning" id="toc-hyperparameter-tuning" class="nav-link" data-scroll-target="#hyperparameter-tuning">8. Hyperparameter Tuning</a></li>
  <li><a href="#assessing-model-performance-for-binary-classification-models" id="toc-assessing-model-performance-for-binary-classification-models" class="nav-link" data-scroll-target="#assessing-model-performance-for-binary-classification-models">9. Assessing Model Performance (For Binary Classification Models)</a>
  <ul class="collapse">
  <li><a href="#binary-classification-nature" id="toc-binary-classification-nature" class="nav-link" data-scroll-target="#binary-classification-nature">1. Binary Classification Nature:</a></li>
  <li><a href="#classification-residual-plot-deviance-residuals" id="toc-classification-residual-plot-deviance-residuals" class="nav-link" data-scroll-target="#classification-residual-plot-deviance-residuals">1. Classification Residual Plot (Deviance Residuals)</a></li>
  </ul></li>
  <li><a href="#model-performance-evaluation" id="toc-model-performance-evaluation" class="nav-link" data-scroll-target="#model-performance-evaluation">10. Model Performance Evaluation</a>
  <ul class="collapse">
  <li><a href="#confusion-matrix" id="toc-confusion-matrix" class="nav-link" data-scroll-target="#confusion-matrix">1. Confusion Matrix</a></li>
  <li><a href="#precision" id="toc-precision" class="nav-link" data-scroll-target="#precision">2. Precision</a></li>
  <li><a href="#recall-sensitivity-or-true-positive-rate" id="toc-recall-sensitivity-or-true-positive-rate" class="nav-link" data-scroll-target="#recall-sensitivity-or-true-positive-rate">3. Recall (Sensitivity or True Positive Rate)</a></li>
  <li><a href="#f1-score" id="toc-f1-score" class="nav-link" data-scroll-target="#f1-score">4. F1-Score</a></li>
  <li><a href="#accuracy" id="toc-accuracy" class="nav-link" data-scroll-target="#accuracy">5. Accuracy</a></li>
  <li><a href="#auc-roc-area-under-the-receiver-operating-characteristic-curve" id="toc-auc-roc-area-under-the-receiver-operating-characteristic-curve" class="nav-link" data-scroll-target="#auc-roc-area-under-the-receiver-operating-characteristic-curve">6. AUC-ROC (Area Under the Receiver Operating Characteristic Curve)</a></li>
  <li><a href="#mean-squared-error-mse" id="toc-mean-squared-error-mse" class="nav-link" data-scroll-target="#mean-squared-error-mse">7. Mean Squared Error (MSE)</a></li>
  <li><a href="#r-squared-r²" id="toc-r-squared-r²" class="nav-link" data-scroll-target="#r-squared-r²">8. R-squared (R²)</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="data-science-study-guide" class="level1">
<h1>Data Science Study Guide</h1>
<section id="stochastic-gradient-descent-sgd" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-gradient-descent-sgd">1. Stochastic Gradient Descent (SGD)</h2>
<ul>
<li><p><strong>Definition</strong>: Stochastic Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning models, particularly for linear models like logistic regression and neural networks. It is a variant of gradient descent where the model parameters are updated for each training example, rather than calculating the gradient based on the entire dataset.</p></li>
<li><p><strong>How it works</strong>:</p>
<ol type="1">
<li><p>Randomly shuffle the data.</p></li>
<li><p>Take one sample from the training set, compute the gradient of the cost function with respect to the model parameters, and update the parameters using this gradient.</p></li>
<li><p>A cost function (e.g.&nbsp;MSE), or <span class="math inline">\(J(\theta)\)</span> is defined: <span class="math display">\[
J(\theta) = \frac{1}{2m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2
\]</span></p></li>
</ol>
<p>where:</p>
<ul>
<li><span class="math inline">\(\hat{y}^{(i)}\)</span> is the model’s prediction for the i-th example,</li>
<li><span class="math inline">\(y^{(i)}\)</span> is the actual value for the i-th example,</li>
<li><span class="math inline">\(\textit{m}\)</span> is the total number of examples,</li>
<li><span class="math inline">\(\theta\)</span> represents the parameters of the model.</li>
</ul>
<ol start="2" type="1">
<li>The gradient is the vector of partial derivatives of the cost function with respect to each model parameter. For each parameter <span class="math inline">\(\theta_j\)</span>, the partial derivative is: <span class="math display">\[
  \frac{\partial J(\theta)}{\partial \theta_j},
  \]</span></li>
</ol>
<p>This derivative is computed to update <span class="math inline">\(\theta_j\)</span>.</p>
<p>Here’s partial derivative with respect to <span class="math inline">\(\theta_j\)</span> : <span class="math display">\[,
  \frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}(\hat{y}^{(i)}-y^{(i)})x_j^{(i)}
  \]</span></p>
<p>where <span class="math inline">\(x_j^{(i)}\)</span> is the value of the <span class="math inline">\(i\)</span>-th example.</p>
<ol start="3" type="1">
<li>The gradient points in the direction of the steepest ascent of the cost function. To minimize the cost function, we take steps in the opposite direction of the gradient. This leads to a parameter update rule in gradient descent:</li>
</ol>
<p><span class="math display">\[
  \theta_j := \theta_j - \alpha\frac{\partial J(\theta)}{\partial \theta_j}
  \]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the learning rate that controls the step size.</p>
<ol start="3" type="1">
<li><p>Repeat this for all samples and iterate until convergence.</p></li>
<li><p>Repeat this for all samples and iterate until convergence.</p></li>
</ol></li>
<li><p><strong>Advantages</strong>:</p>
<ul>
<li>Faster convergence for large datasets.</li>
<li>Reduces memory usage since updates are done incrementally.</li>
</ul></li>
<li><p><strong>Disadvantages</strong>:</p>
<ul>
<li>Can be noisy, as it updates based on individual data points rather than the full batch.</li>
<li>May converge to suboptimal solutions due to noise.</li>
</ul></li>
</ul>
</section>
<section id="multicollinearity" class="level2">
<h2 class="anchored" data-anchor-id="multicollinearity">2. Multicollinearity</h2>
<ul>
<li><p><strong>Definition</strong>: Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, meaning one can be linearly predicted from the others.</p></li>
<li><p><strong>Why it matters</strong>:</p>
<ul>
<li>It can lead to inflated standard errors of the coefficients, making it hard to assess the impact of individual predictors.</li>
<li>Coefficient estimates become unstable and highly sensitive to changes in the model.</li>
</ul></li>
<li><p><strong>Detection Methods</strong>:</p>
<ol type="1">
<li><strong>Variance Inflation Factor (VIF)</strong>: Measures how much the variance of a regression coefficient is inflated due to multicollinearity.</li>
<li><strong>Correlation Matrix</strong>: Check for high correlation values between independent variables.</li>
</ol></li>
<li><p><strong>Solutions</strong>:</p>
<ul>
<li>Remove highly correlated predictors.</li>
<li>Use dimensionality reduction techniques like Principal Component Analysis (PCA).</li>
<li>Use regularization methods like Ridge or Lasso Regression.</li>
</ul></li>
</ul>
</section>
<section id="common-machine-learning-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="common-machine-learning-algorithms">3. Common Machine Learning Algorithms</h2>
<section id="linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression">1. Linear Regression</h3>
<ul>
<li><strong>Type</strong>: Supervised learning (regression)</li>
<li><strong>Use case</strong>: Predicting a continuous value (e.g., predicting house prices).</li>
<li><strong>Key idea</strong>: Models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.</li>
</ul>
</section>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression">2. Logistic Regression</h3>
<ul>
<li><strong>Type</strong>: Supervised learning (classification)</li>
<li><strong>Use case</strong>: Predicting binary outcomes (e.g., spam detection, disease presence).</li>
<li><strong>Key idea</strong>: Uses the logistic function to model binary outcomes, where the output is transformed into probabilities.</li>
</ul>
</section>
<section id="decision-trees" class="level3">
<h3 class="anchored" data-anchor-id="decision-trees">3. Decision Trees</h3>
<ul>
<li><strong>Type</strong>: Supervised learning (classification and regression)</li>
<li><strong>Use case</strong>: Modeling decisions with hierarchical structure (e.g., customer segmentation).</li>
<li><strong>Key idea</strong>: Splits data into subsets based on feature values, leading to a tree-like structure of decisions.</li>
</ul>
</section>
<section id="random-forest" class="level3">
<h3 class="anchored" data-anchor-id="random-forest">4. Random Forest</h3>
<ul>
<li><strong>Type</strong>: Supervised learning (classification and regression)</li>
<li><strong>Use case</strong>: Robust predictions with high accuracy (e.g., fraud detection, image classification).</li>
<li><strong>Key idea</strong>: An ensemble of decision trees, where multiple trees are built and their predictions averaged (regression) or taken by majority vote (classification).</li>
</ul>
</section>
<section id="support-vector-machines-svm" class="level3">
<h3 class="anchored" data-anchor-id="support-vector-machines-svm">5. Support Vector Machines (SVM)</h3>
<ul>
<li><strong>Type</strong>: Supervised learning (classification)</li>
<li><strong>Use case</strong>: High-dimensional classification problems (e.g., text categorization).</li>
<li><strong>Key idea</strong>: Finds the hyperplane that best separates the classes in the feature space.</li>
</ul>
</section>
<section id="k-nearest-neighbors-knn" class="level3">
<h3 class="anchored" data-anchor-id="k-nearest-neighbors-knn">6. K-Nearest Neighbors (KNN)</h3>
<ul>
<li><strong>Type</strong>: Supervised learning (classification and regression)</li>
<li><strong>Use case</strong>: Simple, instance-based learning (e.g., recommendation systems).</li>
<li><strong>Key idea</strong>: Classifies a new data point based on the majority label of its k-nearest neighbors.</li>
</ul>
</section>
<section id="k-means-clustering" class="level3">
<h3 class="anchored" data-anchor-id="k-means-clustering">7. K-Means Clustering</h3>
<ul>
<li><strong>Type</strong>: Unsupervised learning (clustering)</li>
<li><strong>Use case</strong>: Grouping data into clusters (e.g., market segmentation).</li>
<li><strong>Key idea</strong>: Partitions data into k clusters, where each data point belongs to the cluster with the nearest mean.</li>
</ul>
</section>
<section id="gradient-boosting-machines-gbm" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting-machines-gbm">8. Gradient Boosting Machines (GBM)</h3>
<ul>
<li><strong>Type</strong>: Supervised learning (classification and regression)</li>
<li><strong>Use case</strong>: Predictive accuracy in structured data (e.g., financial forecasting).</li>
<li><strong>Key idea</strong>: Builds an ensemble of weak models (e.g., decision trees) in a sequential manner, where each model corrects the errors of its predecessor.</li>
</ul>
</section>
</section>
<section id="regularization" class="level2">
<h2 class="anchored" data-anchor-id="regularization">4. Regularization</h2>
<ul>
<li><p><strong>Definition</strong>: Regularization is a technique to reduce model overfitting by adding a penalty term to the loss function.</p></li>
<li><p><strong>Types</strong>:</p>
<ul>
<li><strong>Ridge Regression (L2 Regularization)</strong>: Adds a penalty equal to the square of the magnitude of coefficients, shrinking less important features but keeping all in the model.</li>
<li><strong>Lasso Regression (L1 Regularization)</strong>: Adds a penalty equal to the absolute value of the magnitude of coefficients, leading to some coefficients becoming exactly zero (automatic feature selection).</li>
</ul></li>
<li><p><strong>Why it matters</strong>: Regularization prevents overfitting by discouraging overly complex models that fit the training data too closely but generalize poorly to new data.</p></li>
</ul>
</section>
<section id="bias-variance-tradeoff" class="level2">
<h2 class="anchored" data-anchor-id="bias-variance-tradeoff">5. Bias-Variance Tradeoff</h2>
<ul>
<li><p><strong>Bias</strong>: Error due to overly simplistic assumptions in the model. High bias can lead to underfitting (the model is too simple to capture the underlying patterns).</p></li>
<li><p><strong>Variance</strong>: Error due to the model being too sensitive to small fluctuations in the training data. High variance can lead to overfitting (the model is too complex and captures noise as well as the underlying patterns).</p></li>
<li><p><strong>Tradeoff</strong>: The goal is to balance bias and variance to minimize the total prediction error.</p></li>
</ul>
</section>
<section id="cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="cross-validation">6. Cross-Validation</h2>
<ul>
<li><p><strong>Definition</strong>: Cross-validation is a technique used to evaluate the generalization ability of a model by splitting the data into multiple subsets (folds) and training the model on some folds while testing on others.</p></li>
<li><p><strong>Types</strong>:</p>
<ul>
<li><strong>K-Fold Cross-Validation</strong>: Divides the dataset into k equally sized folds, trains the model on k-1 folds, and tests on the remaining fold. This is repeated k times, with each fold used exactly once as the test set.</li>
<li><strong>Leave-One-Out Cross-Validation (LOOCV)</strong>: Special case of k-fold cross-validation where k is equal to the number of data points in the dataset.</li>
</ul></li>
<li><p><strong>Why it matters</strong>: Cross-validation helps to assess how well a model will generalize to new, unseen data.</p></li>
</ul>
</section>
<section id="dimensionality-reduction" class="level2">
<h2 class="anchored" data-anchor-id="dimensionality-reduction">7. Dimensionality Reduction</h2>
<ul>
<li><p><strong>Definition</strong>: Techniques used to reduce the number of input variables in a dataset by transforming the data into a lower-dimensional space.</p></li>
<li><p><strong>Key Methods</strong>:</p>
<ul>
<li><strong>Principal Component Analysis (PCA)</strong>: Projects the data onto directions (principal components) that maximize variance.</li>
<li><strong>t-Distributed Stochastic Neighbor Embedding (t-SNE)</strong>: A nonlinear technique for dimensionality reduction used mainly for visualization of high-dimensional datasets.</li>
</ul></li>
<li><p><strong>Why it matters</strong>: Reducing the number of dimensions can improve model performance by removing noise, speeding up computations, and avoiding the curse of dimensionality.</p></li>
</ul>
</section>
<section id="hyperparameter-tuning" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-tuning">8. Hyperparameter Tuning</h2>
<ul>
<li><p><strong>Definition</strong>: Hyperparameters are parameters that are set before the learning process begins and control the behavior of the model. Tuning refers to the process of finding the best set of hyperparameters for optimal model performance.</p></li>
<li><p><strong>Methods</strong>:</p>
<ul>
<li><strong>Grid Search</strong>: Exhaustively tests all combinations of hyperparameter values from a specified set.</li>
<li><strong>Random Search</strong>: Randomly samples hyperparameter values within a specified range, which can be more efficient for high-dimensional spaces.</li>
<li><strong>Bayesian Optimization</strong>: Uses probabilistic models to predict good sets of hyperparameters.</li>
</ul></li>
</ul>
</section>
<section id="assessing-model-performance-for-binary-classification-models" class="level2">
<h2 class="anchored" data-anchor-id="assessing-model-performance-for-binary-classification-models">9. Assessing Model Performance (For Binary Classification Models)</h2>
<section id="binary-classification-nature" class="level3">
<h3 class="anchored" data-anchor-id="binary-classification-nature">1. Binary Classification Nature:</h3>
<ul>
<li>Predicted values in binary classification are often probabilities (between 0 and 1) or the final class labels (0 or 1).</li>
<li>Instead of continuous residuals (differences between predicted and actual values), we deal with probabilities or predicted classes.</li>
</ul>
</section>
<section id="classification-residual-plot-deviance-residuals" class="level3">
<h3 class="anchored" data-anchor-id="classification-residual-plot-deviance-residuals">1. Classification Residual Plot (Deviance Residuals)</h3>
<ul>
<li>Purpose: Shows the difference between the predicted probabilities and the actual binary values.</li>
<li>How to Use: Calculate the residuals based on the predicted probabilities (not class labels) and plot them against the predicted values.</li>
<li>Deviance Residuals: These residuals are used to capture the goodness of fit for binary models. If the model predicts a probability of 0.9 for a true event (actual = 1), the residual will be small. If it predicts 0.1 for the same event, the residual will be large.</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import necessary libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve, roc_curve, confusion_matrix</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.calibration <span class="im">import</span> calibration_curve</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> plot_precision_recall_curve, plot_roc_curve</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data for binary classification</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, n_classes<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into train and test sets</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a Logistic Regression model</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>LogisticRegression()</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict probabilities and class labels</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y_pred_prob <span class="op">=</span> model.predict_proba(X_test)[:, <span class="dv">1</span>]  <span class="co"># Predicted probabilities for class 1</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)  <span class="co"># Predicted class labels</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Residual Plot for Binary Classification</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Residuals = Actual - Predicted Probabilities</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> y_test <span class="op">-</span> y_pred_prob</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>sns.residplot(x<span class="op">=</span>y_pred_prob, y<span class="op">=</span>residuals, lowess<span class="op">=</span><span class="va">True</span>, line_kws<span class="op">=</span>{<span class="st">'color'</span>: <span class="st">'red'</span>})</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted Probabilities'</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Residuals (Actual - Predicted Probabilities)'</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Residual Plot for Binary Classification'</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="output_28_0.png" class="img-fluid figure-img"></p>
<figcaption>png</figcaption>
</figure>
</div>
<p>The red plot in the graphs above represent the LOESS curve. In binary classification, residuals are the differences between the predicted probabilities and the actual labels (0 or 1). When you plot the residuals using a LOESS (Locally Weighted Scatterplot Smoothing) or LOWESS curve, you’re looking for patterns that can indicate the performance of your model across different ranges of predicted probabilities.</p>
<p>Here’s how to interpret the plot:</p>
<ol type="1">
<li><strong>A flat LOESS curve near zero</strong>:</li>
</ol>
<p>If the LOESS curve is flat and close to zero, this indicates that your model is well-calibrated. The residuals are evenly distributed across the predicted probabilities, meaning the model is making accurate predictions without any systematic bias. 2. <strong>Upward or downward trends:</strong></p>
<p>If the curve trends upward or downward, this could indicate bias in the model’s predictions. Upward trend: Your model is underestimating probabilities for higher predicted values (i.e., predicted probabilities are too low). Downward trend: Your model is overestimating probabilities for higher predicted values (i.e., predicted probabilities are too high). 3. <strong>Bowl- or U-shaped curve:</strong></p>
<p>A U-shaped or bowl-shaped LOESS curve suggests that the model is overconfident in extreme predictions (close to 0 or 1), but less accurate for predictions near 0.5. This is common in binary classification models that tend to overfit or make extreme probability predictions in both classes. 4. <strong>S-shaped curve:</strong></p>
<p>An S-shaped curve might indicate non-linearity in the relationship between features and the target. The model might need to capture more complex patterns, possibly hinting that you could benefit from a more complex model or feature engineering. 5. <strong>Variance around the curve:</strong></p>
<p>High variance or scattered points around the LOESS curve indicate heteroscedasticity (unequal spread of residuals). Ideally, the residuals should be uniformly spread around the LOESS curve, suggesting that the model performs equally well across all predicted probabilities. In summary, you want the LOWESS plot to be as flat and close to zero as possible, indicating minimal bias and good calibration of the model across all ranges of predicted probabilities. Trends or shapes in the plot suggest areas for improvement, such as model calibration or addressing bias.</p>
</section>
</section>
<section id="model-performance-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="model-performance-evaluation">10. Model Performance Evaluation</h2>
<p>When building machine learning models, it’s important to evaluate their performance using appropriate metrics. The choice of metric depends on the type of problem (classification, regression, etc.) and the goals of the analysis.</p>
<section id="confusion-matrix" class="level3">
<h3 class="anchored" data-anchor-id="confusion-matrix">1. Confusion Matrix</h3>
<ul>
<li><strong>Definition</strong>: A confusion matrix is a table used to evaluate the performance of a classification model by comparing actual vs.&nbsp;predicted values. It consists of four components:
<ul>
<li><strong>True Positives (TP)</strong>: Correctly predicted positive instances.</li>
<li><strong>True Negatives (TN)</strong>: Correctly predicted negative instances.</li>
<li><strong>False Positives (FP)</strong>: Incorrectly predicted positive instances (Type I error).</li>
<li><strong>False Negatives (FN)</strong>: Incorrectly predicted negative instances (Type II error).</li>
</ul></li>
<li><strong>Example</strong>: A confusion matrix for a binary classification model might look like this:</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>Predicted Positive</th>
<th>Predicted Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Actual Positive</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr class="even">
<td>Actual Negative</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody>
</table>
</section>
<section id="precision" class="level3">
<h3 class="anchored" data-anchor-id="precision">2. Precision</h3>
<ul>
<li><p><strong>Definition</strong>: Precision is the ratio of correctly predicted positive observations to the total predicted positives. It answers the question: <em>Of all instances classified as positive, how many were actually positive?</em></p></li>
<li><p><strong>Formula</strong>: <span class="math display">\[
\text{Precision} = \frac{TP}{TP + FP}
\]</span></p></li>
<li><p><strong>Interpretation</strong>: High precision means the model makes fewer false positive predictions. It is important when the cost of false positives is high, such as in medical diagnostics.</p></li>
</ul>
</section>
<section id="recall-sensitivity-or-true-positive-rate" class="level3">
<h3 class="anchored" data-anchor-id="recall-sensitivity-or-true-positive-rate">3. Recall (Sensitivity or True Positive Rate)</h3>
<ul>
<li><p><strong>Definition</strong>: Recall is the ratio of correctly predicted positive observations to all actual positives. It answers the question: <em>Of all actual positive instances, how many were correctly predicted?</em></p></li>
<li><p><strong>Formula</strong>: <span class="math display">\[
\text{Recall} = \frac{TP}{TP + FN}
\]</span></p></li>
<li><p><strong>Interpretation</strong>: High recall indicates that the model captures most of the positive instances, but it might have more false positives. Recall is critical in scenarios where missing a positive case is more costly, such as in disease detection.</p></li>
</ul>
</section>
<section id="f1-score" class="level3">
<h3 class="anchored" data-anchor-id="f1-score">4. F1-Score</h3>
<ul>
<li><p><strong>Definition</strong>: The F1-score is the harmonic mean of precision and recall. It provides a single metric that balances the trade-off between precision and recall.</p></li>
<li><p><strong>Formula</strong>: <span class="math display">\[
\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]</span></p></li>
<li><p><strong>Interpretation</strong>: The F1-score is useful when we need a balance between precision and recall, especially in cases where the class distribution is imbalanced.</p></li>
</ul>
</section>
<section id="accuracy" class="level3">
<h3 class="anchored" data-anchor-id="accuracy">5. Accuracy</h3>
<ul>
<li><p><strong>Definition</strong>: Accuracy is the ratio of correctly predicted observations to the total observations.</p></li>
<li><p><strong>Formula</strong>: <span class="math display">\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]</span></p></li>
<li><p><strong>Interpretation</strong>: Accuracy is a good metric when the classes are balanced, but it can be misleading when the data is imbalanced. For instance, if 95% of instances are negative, a model that predicts everything as negative will have high accuracy but poor precision and recall.</p></li>
</ul>
</section>
<section id="auc-roc-area-under-the-receiver-operating-characteristic-curve" class="level3">
<h3 class="anchored" data-anchor-id="auc-roc-area-under-the-receiver-operating-characteristic-curve">6. AUC-ROC (Area Under the Receiver Operating Characteristic Curve)</h3>
<ul>
<li><p><strong>Definition</strong>: The ROC curve is a plot of the true positive rate (recall) against the false positive rate (1 - specificity). The area under the curve (AUC) provides a single metric that represents the overall performance of the model.</p></li>
<li><p><strong>Interpretation</strong>: AUC-ROC values range from 0.5 (random guessing) to 1.0 (perfect classifier). It is useful for evaluating models on imbalanced datasets because it takes both positive and negative classes into account.</p></li>
</ul>
</section>
<section id="mean-squared-error-mse" class="level3">
<h3 class="anchored" data-anchor-id="mean-squared-error-mse">7. Mean Squared Error (MSE)</h3>
<ul>
<li><p><strong>Definition</strong>: MSE is used to evaluate regression models. It is the average of the squared differences between actual and predicted values.</p></li>
<li><p><strong>Formula</strong>: <span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</span></p></li>
<li><p><strong>Interpretation</strong>: A lower MSE indicates a better model fit. However, since it squares the errors, larger errors are penalized more heavily.</p></li>
</ul>
</section>
<section id="r-squared-r²" class="level3">
<h3 class="anchored" data-anchor-id="r-squared-r²">8. R-squared (R²)</h3>
<ul>
<li><p><strong>Definition</strong>: R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model.</p></li>
<li><p><strong>Formula</strong>: <span class="math display">\[
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\]</span></p></li>
<li><p><strong>Interpretation</strong>: R² values range from 0 to 1. A higher R² value indicates a better fit of the model to the data. An R² of 0 means the model does not explain any of the variance, while an R² of 1 means the model explains all the variance.</p></li>
</ul>


</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{young2025,
  author = {Young, Therin},
  title = {My {Data} {Science} {Study} {Guide}},
  date = {2025-05-07},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-young2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Young, Therin. 2025. <span>“My Data Science Study Guide.”</span> May 7,
2025.
</div></div></section></div></main> <!-- /main -->
<div>

    <hr>

    

    <h4> Stay in touch </h4>

    

    <iframe src="https://embeds.beehiiv.com/cb95c556-8bf4-4ed1-865d-f20fdb642a1a?slim=true" data-test-id="beehiiv-embed" height="52" frameborder="0" scrolling="no" style="margin: 0; border-radius: 0px !important; background-color: transparent;"><script type="text/javascript" async src="https://embeds.beehiiv.com/attribution.js"></script></iframe>



    

    <script src="https://storage.ko-fi.com/cdn/scripts/overlay-widget.js"></script>

    <script>

      kofiWidgetOverlay.draw('therin229', {

        'type': 'floating-chat',

        'floating-chat.donateButton.text': 'Support me',

        'floating-chat.donateButton.background-color': '#00b9fe',

        'floating-chat.donateButton.text-color': '#fff'

      });

    </script>    

    <hr>

    </div>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>