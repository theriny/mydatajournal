[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "my data journal",
    "section": "",
    "text": "Stay in touch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Data Science Study Guide\n\n\n\ndata science\n\ninterviewing\n\n\n\nA place for all of the Data Science basics\n\n\n\nTherin Young\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogging Into Nova HPC Cluster\n\n\n\npython\n\nmicromamba\n\nHPC\n\n\n\nWriting this down to keep future me sane\n\n\n\nTherin Young\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1: Exploring Fulton County‚Äôs Subdivision Data\n\n\n\npython\n\nhousing development\n\nreal estate\n\n\n\nExploring subdivision data as it relates to the shape area and length of the subdivisions\n\n\n\nTherin Young\n\n\nMay 1, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Welcome",
    "section": "",
    "text": "üëã Hi, I‚Äôm Therin!\nI‚Äôm a data scientist passionate about building tools that make data more accessible and actionable. I earned my master‚Äôs in mechanical engineering from Iowa State University in 2016, and I‚Äôm currently a PhD candidate using machine learning and 3D modeling to build software solutions for precision agriculture applications. I use Python, Azure, and SQL in my current role.\nI spent 2 years at Ford Motor Company as a data scientist on the Material Cost & Analytics team where I used Python, Qlik, and Alteryx to build analytics dashboards and forecasting tools to optimize vehicle feature scheduling and reduce revenue loss. I also spent a year with Collaborative Real Estate gathering insight from FLASH parking data to improve the parker experience.\nWhen I‚Äôm not out playing soccer with my two boys or spending time with my wife, I‚Äôm producing beats on my Maschine, playing bass guitar, or getting owned by teenagers in Fortnite.\nYou can find some of my work on my Github.\nHere‚Äôs a quick animation of how I use 3D modeling and Python to automate the extraction of features (phenotypes) from soybean canopies. https://www.collabre.co/\n\n\nYour browser does not support the video tag. \n\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats GitHub Repoüßëüèæ‚ÄçüíªLinkedIn Article‚úçüèæ"
  },
  {
    "objectID": "projects.html#forecasts",
    "href": "projects.html#forecasts",
    "title": "Projects",
    "section": "",
    "text": "2025 March Madness: A Bayesian forecast of the 2025 men‚Äôs and women‚Äôs March Madness tournaments (men‚Äôs forecast | women‚Äôs forecast | repo)"
  },
  {
    "objectID": "projects.html#talks-publications-etc.",
    "href": "projects.html#talks-publications-etc.",
    "title": "Projects",
    "section": "Talks, Publications, etc.",
    "text": "Talks, Publications, etc."
  },
  {
    "objectID": "scholarship.html",
    "href": "scholarship.html",
    "title": "Scholarship",
    "section": "",
    "text": "üìö Publications\nYoung, T.; Chiranjeevi, S.; Elango, D.; Sarkar, S.; Singh, A.K.; Singh, A.; Ganapathysubramanian, B.; Jubery, T.Z.\nSoybean Canopy Stress Classification Using 3D Point Cloud Data. Agronomy 2024, 14, 1181.\nhttps://doi.org/10.3390/agronomy14061181\n\nYoung TJ, Jubery TZ, Carley CN, Carroll M, Sarkar S, Singh AK, Singh A, Ganapathysubramanian B.\n‚ÄúCanopy fingerprints‚Äù for characterizing three-dimensional point cloud data of soybean canopies. Front. Plant Sci. 2023;14:1141153.\nhttps://doi.org/10.3389/fpls.2023.1141153\n\nSingh, A.K., Singh, A., Young, T.\nHigh-Throughput Phenotyping in Soybean. In: High-Throughput Crop Phenotyping. Springer, 2021. pp.¬†129‚Äì163.\nhttps://link.springer.com/chapter/10.1007/978-3-030-82574-1_6\n\nMirnezami, V., Young, T., Assefa, T., Prichard, S., Nagasubramanian, K., Sandhu, K., Sarkar, S., Sundararajan, S., O‚ÄôNeal, M., Ganapathysubramanian, B., Singh, A.\nTrichome density measurement in soybean leaflet using advanced image processing techniques. Applications in Plant Sciences, 2020; 8(7): e11375.\nhttps://doi.org/10.1002/aps3.11375\n\nYoung, T., Jackson, J., Roy, S., Ceylan, H., Sundararajan, S.\nTribological behavior and wettability of spray-coated superhydrophobic coatings on aluminum. Wear, 2017; 376‚Äì377 (Part B): 1713‚Äì1719.\nhttps://doi.org/10.1016/j.wear.2016.12.050\n\nYoung, T.\nDevelopment of durable superhydrophobic materials for ice- and snow-free airport concrete pavements. M.S. Thesis, Iowa State University, 2016.\nhttps://dr.lib.iastate.edu/entities/publication/9cf1832e-44cb-4dd5-970b-92a97c0c787b"
  },
  {
    "objectID": "projects.html#sports-ball",
    "href": "projects.html#sports-ball",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats repo"
  },
  {
    "objectID": "projects.html#sport",
    "href": "projects.html#sport",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats repo"
  },
  {
    "objectID": "projects.html#sports",
    "href": "projects.html#sports",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats GitHub Repoüßëüèæ‚ÄçüíªLinkedIn Article‚úçüèæ"
  },
  {
    "objectID": "projects.html#music",
    "href": "projects.html#music",
    "title": "Projects",
    "section": "Music üéµüé∏üéß",
    "text": "Music üéµüé∏üéß\n\nGenius Lyric Scraperüé∂: Using Python and Genius‚Äôs API tool to scrape song lyrics GitHub Repoüßëüèæ‚Äçüíª Thinking of creating a Python library for this one"
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "üëã Hi, I‚Äôm Therin!",
    "section": "",
    "text": "üëã Hi, I‚Äôm Therin!\nI‚Äôm a data scientist passionate about building tools that make data more accessible and actionable. I earned my master‚Äôs in mechanical engineering from Iowa State University in 2016, and I‚Äôm currently a PhD candidate using machine learning and 3D modeling to build software solutions for precision agriculture applications. I use Python, Azure, and SQL in my current role.\nI spent 2 years at Ford Motor Company as a data scientist on the Material Cost & Analytics team where I used Python, Qlik, and Alteryx to build analytics dashboards and forecasting tools to optimize vehicle feature scheduling and reduce revenue loss. I also spent a year with Collaborative Real Estate gathering insight from FLASH parking data to improve the parker experience.\nWhen I‚Äôm not out playing soccer with my two boys or spending time with my wife, I‚Äôm producing beats on my Maschine, playing bass guitar, or getting owned by teenagers in Fortnite.\nYou can find some of my work on my Github.\nHere‚Äôs a quick animation of how I use 3D modeling and Python to automate the extraction of features (phenotypes) from soybean canopies.\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "scholarship/scholarship.html",
    "href": "scholarship/scholarship.html",
    "title": "Scholarship",
    "section": "",
    "text": "üìö Publications\nYoung, T.; Chiranjeevi, S.; Elango, D.; Sarkar, S.; Singh, A.K.; Singh, A.; Ganapathysubramanian, B.; Jubery, T.Z.\nSoybean Canopy Stress Classification Using 3D Point Cloud Data. Agronomy 2024, 14, 1181.\nhttps://doi.org/10.3390/agronomy14061181\n\nYoung TJ, Jubery TZ, Carley CN, Carroll M, Sarkar S, Singh AK, Singh A, Ganapathysubramanian B.\n‚ÄúCanopy fingerprints‚Äù for characterizing three-dimensional point cloud data of soybean canopies. Front. Plant Sci. 2023;14:1141153.\nhttps://doi.org/10.3389/fpls.2023.1141153\n\nSingh, A.K., Singh, A., Young, T.\nHigh-Throughput Phenotyping in Soybean. In: High-Throughput Crop Phenotyping. Springer, 2021. pp.¬†129‚Äì163.\nhttps://link.springer.com/chapter/10.1007/978-3-030-82574-1_6\n\nMirnezami, V., Young, T., Assefa, T., Prichard, S., Nagasubramanian, K., Sandhu, K., Sarkar, S., Sundararajan, S., O‚ÄôNeal, M., Ganapathysubramanian, B., Singh, A.\nTrichome density measurement in soybean leaflet using advanced image processing techniques. Applications in Plant Sciences, 2020; 8(7): e11375.\nhttps://doi.org/10.1002/aps3.11375\n\nYoung, T., Jackson, J., Roy, S., Ceylan, H., Sundararajan, S.\nTribological behavior and wettability of spray-coated superhydrophobic coatings on aluminum. Wear, 2017; 376‚Äì377 (Part B): 1713‚Äì1719.\nhttps://doi.org/10.1016/j.wear.2016.12.050\n\nYoung, T.\nDevelopment of durable superhydrophobic materials for ice- and snow-free airport concrete pavements. M.S. Thesis, Iowa State University, 2016.\nhttps://dr.lib.iastate.edu/entities/publication/9cf1832e-44cb-4dd5-970b-92a97c0c787b"
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats GitHub Repoüßëüèæ‚ÄçüíªLinkedIn Article‚úçüèæ"
  },
  {
    "objectID": "projects/projects.html#sports",
    "href": "projects/projects.html#sports",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats GitHub Repoüßëüèæ‚ÄçüíªLinkedIn Article‚úçüèæ"
  },
  {
    "objectID": "projects/projects.html#music",
    "href": "projects/projects.html#music",
    "title": "Projects",
    "section": "Music üéµüé∏üéß",
    "text": "Music üéµüé∏üéß\n\nGenius Lyric Scraperüé∂: Using Python and Genius‚Äôs API tool to scrape song lyrics GitHub Repoüßëüèæ‚Äçüíª Thinking of creating a Python library for this one"
  },
  {
    "objectID": "posts/5_1_2025_SFulton_subdivisions/post.html",
    "href": "posts/5_1_2025_SFulton_subdivisions/post.html",
    "title": "Part 1: Exploring Fulton County‚Äôs Subdivision Data",
    "section": "",
    "text": "This is part 1 of a 2- (maybe 3) part analysis of Fulton County, Georgia‚Äôs subdivisions, where the only metrics recorded are shape area and shape lengthüìêüìè (A little more detail about those metrics later in the analysis). Doesn‚Äôt seem like a very interesting dataset at first glanceü•∏, but I always uncover interesting and unexpected nuggetsü™ô of information from seemingly dull datasets. So, if I find a rabbit hole, let‚Äôs hope it‚Äôs a fun one.üê∞\nI‚Äôm going to start the first part of the series from far outüõ∞Ô∏è (basic statistics) and gradually hone in for a more nuanced analysisüî¨ as the series progresses and I begin to ask questions about the data that is relevant to my own location in South Fulton.\nI‚Äôm going to place this series in Housing and DevelopmentüèòÔ∏è, so readers who are experts or simply enthusiasts of the field can feel free to offer suggestions, comments, and any version of personal opinion on the matter. I‚Äôd love to hear other takes and approaches to extracting meaning from this type of dataset.\nHappy Exploring! üèîÔ∏è\nData Source: https://gisdata.fultoncountyga.gov/ GitHub Repo: https://github.com/theriny/fultoncounty_subdivisions\n\nGet Dependencies\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n\nRead Data\ndf = pd.read_csv('Subdivisions.csv')\ndf.describe()\n\n\n\n\n\n\n\n\nOBJECTID\n\n\nYearRecorded\n\n\nDocBook\n\n\nDocPage\n\n\nShape__Area\n\n\nShape__Length\n\n\n\n\n\n\ncount\n\n\n3677.000000\n\n\n1823.000000\n\n\n83.000000\n\n\n83.000000\n\n\n3.675000e+03\n\n\n3675.000000\n\n\n\n\nmean\n\n\n2081.452543\n\n\n2002.259462\n\n\n313.493976\n\n\n79.000000\n\n\n1.107878e+06\n\n\n4214.630614\n\n\n\n\nstd\n\n\n1918.580076\n\n\n26.901355\n\n\n131.934978\n\n\n41.136596\n\n\n2.252761e+06\n\n\n3748.502907\n\n\n\n\nmin\n\n\n1.000000\n\n\n978.000000\n\n\n99.000000\n\n\n1.000000\n\n\n3.198242e-02\n\n\n16.750267\n\n\n\n\n25%\n\n\n941.000000\n\n\n1995.000000\n\n\n183.500000\n\n\n44.000000\n\n\n1.702545e+05\n\n\n1867.852593\n\n\n\n\n50%\n\n\n1876.000000\n\n\n2004.000000\n\n\n333.000000\n\n\n75.000000\n\n\n5.075356e+05\n\n\n3355.910444\n\n\n\n\n75%\n\n\n2799.000000\n\n\n2013.000000\n\n\n445.500000\n\n\n114.500000\n\n\n1.218273e+06\n\n\n5457.816465\n\n\n\n\nmax\n\n\n14872.000000\n\n\n2023.000000\n\n\n464.000000\n\n\n148.000000\n\n\n4.476438e+07\n\n\n55177.479838\n\n\n\n\n\nThis dataset contains the shape area and shape length of subdivisions for various cities in Fulton County, Georgia that span from 978 to 2023. 978!?!?üò≤ It seems we may have a typo. Let‚Äôs look at the unique years that were recorded.\n\n\nA little tidying up\nYea, let‚Äôs change 978 to 1978. How many times was this typo made?\ndf[df['YearRecorded'] == 978.]\n\n\n\n\n\n\n\n\nOBJECTID\n\n\nSubdivName\n\n\nCreatedBy\n\n\nCreateDate\n\n\nEditor\n\n\nLastEdit\n\n\nCity\n\n\nFeatureID\n\n\nYearRecorded\n\n\nDocBook\n\n\nDocPage\n\n\nDocType\n\n\nShape__Area\n\n\nShape__Length\n\n\n\n\n\n\n3642\n\n\n13636\n\n\nCameron Crest Farms\n\n\nSTEPHANIE.YANCEY\n\n\n2023/10/17 12:07:11+00\n\n\nSTEPHANIE.YANCEY\n\n\n2023/10/17 12:08:24+00\n\n\nJohns Creek\n\n\nLSD0000413\n\n\n978.0\n\n\n113.0\n\n\n4.0\n\n\nPL\n\n\n5.919767e+06\n\n\n11584.65774\n\n\n\n\n\nOnly once on row 3642. So let‚Äôs make a quick fix using pandas .loc function.\n# replace 978 in row 3642 with 1978\ndf.loc[3642,'YearRecorded'] = 1978\n#sort record dates\nnp.sort(df['YearRecorded'].unique())\narray([1939., 1961., 1968., 1970., 1971., 1972., 1973., 1974., 1975.,\n       1976., 1977., 1978., 1979., 1980., 1981., 1982., 1983., 1984.,\n       1985., 1986., 1987., 1988., 1989., 1990., 1991., 1992., 1993.,\n       1994., 1995., 1996., 1997., 1998., 1999., 2000., 2001., 2002.,\n       2003., 2004., 2005., 2006., 2007., 2008., 2009., 2010., 2011.,\n       2012., 2013., 2014., 2015., 2016., 2017., 2018., 2019., 2020.,\n       2021., 2022., 2023.,   nan])\nSince the YearRecorded field will be used later down the line, I want to remove any rows with nan in this column. How many of these rows are there?\n(df[np.isnan(df['YearRecorded']) == True].shape[0])/df.shape[0]\n0.5042153929834103\ndf.shape\n(3677, 14)\nThat‚Äôs 50% of the dataset üò≤. Now, there are two columns in the dataset: CreateDate and LastEdit, which have the same date value and seem to be dates related to when the editor of the dataset, Stephanie Yancey (Column: CreatedBy) edited or entered the data, so not necessarily the original date of when the information was first recorded. I could replace the nan dates of YearRecorded with one these dates, but that could lead to faulty results when I want to look at things like what was the average subdivision shape area in 2023?, since the majority of Stephanie Yancey‚Äôs edits were in 2023.\nSooo, I‚Äôm going to remove the nan rows (for now).\n# remove rows where YearRecorded is nan\ndf = df[np.isnan(df['YearRecorded']) != True]\n# confirm rows were removed\ndf.shape\n(1823, 14)\nLet‚Äôs also convert shape area from square-feet to acres. I‚Äôm guessing the current values for shape area are in square feet, and shape length is in feet. üìèüìê\n# convert square feet to acres (43,560 square feet is 1 acre)\ndf['Shape__Area'] = df['Shape__Area'].apply(lambda x: x/43560.0)\n\n\nExploration üß≠üîç\nAwesome!üôÜüèæ So, the subdivision shape area and length data recordings range from 1939 to 2023. I‚Äôm curious to know if the area and/or length of a subdivision can change over time. I‚Äôm sure it can if say, a subdivision is rezoned due to home reductions or additions, but I want to know if those events are captured in the data. I‚Äôll create a function that will essentially: 1. Determine if a subdivision name is in more than one row of the dataset.üí≠(duplicate entry OR same subdiv name but different city OR same subdiv name and city but different shape area/length and record dates (the gold ü™ô) 2. Determine if the subdivision‚Äôs city is the same for each additional appearance of the subdivision (if it‚Äôs not, this probably means the cities have subdivisions with the same name, which is common) üí≠ 3. The function will return a table that lists all subdivisions that meet the above criteria.üí≠\n# üßëüèæ‚ÄçüíªCreate function that finds subdivisions with more than 1 record where the measurement and/or year are different between the records\n\ndef Subdivisions_With_Updates(dataframe):\n    \n    \n    # create empty lists to capture city, year, shape, subdivname\n    city = []\n    year = []\n    shape = []\n    subdivname = []\n    \n    \n    for subdiv in dataframe['SubdivName'].unique():\n        \n        # get subset of dataframe only containing current subdivision\n        uniqueSubs = dataframe[dataframe['SubdivName'] == subdiv]\n        \n        # get the unique cities associated with current subdivision\n        uniqueCity = dataframe[dataframe['SubdivName'] == subdiv]['City'].unique()\n        \n        if (len(uniqueSubs) &gt; 1) & (len(uniqueCity) == 1): #if the subdivision appears more than once and there is one unique city, this indicates a change in the subdivisions recorded area/length\n            for i in range(0,len(uniqueSubs)):\n                uniqueSubs = uniqueSubs.reset_index(drop=True) # reset index\n                subdivname.append(uniqueSubs['SubdivName'][i])\n                city.append(uniqueSubs['City'][i])\n                year.append(uniqueSubs['YearRecorded'][i])\n                shape.append(uniqueSubs['Shape__Area'][i])\n            \n        else:\n            None\n    results = pd.DataFrame({'SubDivName': subdivname, 'City': city, 'Year': year, 'Shape Area': shape})\n    \n    return results      \n# Run the function and save results to 'results' variable\nresults = Subdivisions_With_Updates(df)\nresults.head()\n\n\n\n\n\n\n\n\nSubDivName\n\n\nCity\n\n\nYear\n\n\nShape Area\n\n\n\n\n\n\n0\n\n\nDPMS Builders LLC Phase 3\n\n\nAtlanta\n\n\n2017.0\n\n\n0.246343\n\n\n\n\n1\n\n\nDPMS Builders LLC Phase 3\n\n\nAtlanta\n\n\n2016.0\n\n\n1.868496\n\n\n\n\n2\n\n\nOverton Hills\n\n\nSandy Springs\n\n\n1978.0\n\n\n17.816186\n\n\n\n\n3\n\n\nOverton Hills\n\n\nSandy Springs\n\n\n1978.0\n\n\n15.884354\n\n\n\n\n4\n\n\nCameron Crest Farms\n\n\nJohns Creek\n\n\n1978.0\n\n\n219.386157\n\n\n\n\n\n# get unique subdivisions whose shape area has at some point in time\nlen(list(results['SubDivName'].unique()))\n15\n# get unique subdivisions in dataset\ndf.groupby(by=['SubdivName','City']).ngroups\n1791\nlen(list(df['SubdivName'].unique()))\n1803\nWe can see üôà that 21 of the subdivisions‚Äô shape size has changed at some point in time. Thats 21 out of 3,628 unique subdivisions in the dataset (taking into account that some cities share subdivivision names)‚Ä¶That‚Äôs less than 1%ü§èüèæ. This indicates that subdivisions are rarely reduced or increased in size after they have been developed (in Fulton County, GA).\nI‚Äôm also curious to know if the average shape area of subdivisions has evolved over the years.\n# calculate the average subdivision shape area for each year\nshapeArea_perYear = df.groupby(by=['YearRecorded']).agg({'Shape__Area': 'mean'})\nplt.bar(shapeArea_perYear.index,shapeArea_perYear.Shape__Area)\nplt.ylabel('Shape Area (acres)')\nplt.xlabel('YearRecorded')\nplt.show()\nText(0.5, 0, 'YearRecorded')\n\n\n\npng\n\n\nIt‚Äôs interesting to note that there was a 20-year gap between 1940 and 1960 where no data was reported. üòØ\nWhat subdivisions existed in 1939 in Fulton County, GA???\ndf[df['YearRecorded'] == 1939.0]\n\n\n\n\n\n\n\n\nOBJECTID\n\n\nSubdivName\n\n\nCreatedBy\n\n\nCreateDate\n\n\nEditor\n\n\nLastEdit\n\n\nCity\n\n\nFeatureID\n\n\nYearRecorded\n\n\nDocBook\n\n\nDocPage\n\n\nDocType\n\n\nShape__Area\n\n\nShape__Length\n\n\n\n\n\n\n2923\n\n\n2965\n\n\nBroad Subdivision\n\n\nCHRIS.MCMILLER\n\n\n2019/03/28 14:36:37+00\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nSouth Fulton\n\n\nLSD0000329\n\n\n1939.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n200.350853\n\n\n20924.700663\n\n\n\n\n\nBroad Subdivisionü§î I‚Äôm guessing this is a collection of subdivisions rolled into a single datapoint because it‚Äôs just weird to think a single subdivision was that large in 1939 and non-existent 20 years later after no data was recorded for 20 years. It‚Äôs fair to say data recording didn‚Äôt become more consistent until the 1970‚Äôs, so it might be fair cut datapoints earlier than 1970 moving forward. There are, however, additional peaks in shape area for 4 or 5 of the years after 1970 that should be investigated.\nWhat about subdivision shape length?\n#calculate average subdivision shape length per year\nshapeLength_perYear = df.groupby(by='YearRecorded').agg({'Shape__Length':'mean'})\n\nplt.bar(shapeLength_perYear.index,shapeLength_perYear.Shape__Length)\nplt.xlabel('YearRecorded')\nplt.ylabel('Shape Length (ft)')\nplt.show()\nText(0, 0.5, 'Shape Length (ft)')\n\n\n\npng\n\n\nLet‚Äôs end the first part of the series by looking at the top 5Ô∏è‚É£ subdivisions with the largest shape areas. But lets first remove any recordings before 1970.\n# get records from 1970 and up\ndf_post1970 = df[df['YearRecorded'] &gt; 1969.0]\nsorted_post1970 = df_post1970.sort_values(by='Shape__Area',ascending=False)\nsorted_post1970.head()\n\n\n\n\n\n\n\n\nOBJECTID\n\n\nSubdivName\n\n\nCreatedBy\n\n\nCreateDate\n\n\nEditor\n\n\nLastEdit\n\n\nCity\n\n\nFeatureID\n\n\nYearRecorded\n\n\nDocBook\n\n\nDocPage\n\n\nDocType\n\n\nShape__Area\n\n\nShape__Length\n\n\n\n\n\n\n518\n\n\n539\n\n\nMartins Landing/Lakeview Homes\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nRoswell\n\n\nLSD0001546\n\n\n1981.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n1027.648744\n\n\n34842.968347\n\n\n\n\n3373\n\n\n3415\n\n\nHorseshoe Bend\n\n\nDALU.FAB-UKOZOR\n\n\n2020/07/30 14:08:32+00\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nNaN\n\n\nLSD0001239\n\n\n1994.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n860.696880\n\n\n40867.862487\n\n\n\n\n461\n\n\n482\n\n\nWillow Springs\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nRoswell\n\n\nLSD0002852\n\n\n1985.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n581.907403\n\n\n26407.221626\n\n\n\n\n668\n\n\n689\n\n\nSaddle Creek\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nRoswell\n\n\nLSD0002204\n\n\n1985.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n431.206870\n\n\n18912.245659\n\n\n\n\n2640\n\n\n2682\n\n\nCedar Grove Village Phase 1 Section A\n\n\nCHRIS.MCMILLER\n\n\n2018/12/20 12:25:46+00\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:54:34+00\n\n\nSouth Fulton\n\n\nLSD0000483\n\n\n2003.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n377.121148\n\n\n21131.509310\n\n\n\n\n\nsns.barplot(data=sorted_post1970.iloc[:5], x = 'SubdivName',y= 'Shape__Area')\nplt.ylabel('Shape Area (acres)')\nplt.xlabel('Subdivision')\nplt.xticks(rotation=90)\nplt.show()\n(array([0, 1, 2, 3, 4]),\n [Text(0, 0, 'Martins Landing/Lakeview Homes'),\n  Text(1, 0, 'Horseshoe Bend'),\n  Text(2, 0, 'Willow Springs'),\n  Text(3, 0, 'Saddle Creek'),\n  Text(4, 0, 'Cedar Grove Village Phase 1 Section A')])\n\n\n\npng\n\n\nAnd, that is an interesting place to end. The 5th largest subdivision shape area happens to be the subdivision I live in, at about 400 acres! üò≤üôÇ\nThe next installment to this series is coming soon. Until then, Happy Exploring.\n\n\n\n\nCitationBibTeX citation:@online{young2025,\n  author = {Young, Therin},\n  title = {Part 1: {Exploring} {Fulton} {County‚Äôs} {Subdivision} {Data}},\n  date = {2025-05-01},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nYoung, Therin. 2025. ‚ÄúPart 1: Exploring Fulton County‚Äôs\nSubdivision Data.‚Äù May 1, 2025."
  },
  {
    "objectID": "posts/5_1_2025_SFulton_subdivisions/post.html#data-cleaningprocessing",
    "href": "posts/5_1_2025_SFulton_subdivisions/post.html#data-cleaningprocessing",
    "title": "Journal",
    "section": "Data Cleaning/Processing",
    "text": "Data Cleaning/Processing\n#convert shape_area fild to integer values\ndf['Shape__Area'] = df['Shape__Area'].apply(lambda x: int(x) if not pd.isna(x) else x)\ndf.head()\n\n\n\n\n\n\n\n\nOBJECTID\n\n\nSubdivName\n\n\nCreatedBy\n\n\nCreateDate\n\n\nEditor\n\n\nLastEdit\n\n\nCity\n\n\nFeatureID\n\n\nYearRecorded\n\n\nDocBook\n\n\nDocPage\n\n\nDocType\n\n\nShape__Area\n\n\nShape__Length\n\n\n\n\n\n\n0\n\n\n1\n\n\nPeachtree Gardens\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nAtlanta\n\n\nLSD0001908\n\n\n1995.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n694869.0\n\n\n5831.032211\n\n\n\n\n1\n\n\n2\n\n\nSpringlake Park/spring Lake\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nAtlanta\n\n\nLSD0002333\n\n\n1995.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n427291.0\n\n\n2957.645702\n\n\n\n\n2\n\n\n3\n\n\nAwesome Homes Inc\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nAtlanta\n\n\nLSD0000188\n\n\n1995.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n184450.0\n\n\n1855.649760\n\n\n\n\n3\n\n\n4\n\n\nColumbia Residential of Mechanicsville Residen‚Ä¶\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:22:32+00\n\n\nAtlanta\n\n\nLSD0000590\n\n\n2015.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n140338.0\n\n\n2290.047939\n\n\n\n\n4\n\n\n5\n\n\nThe Central Buckhead/twnhs\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nAtlanta\n\n\nLSD0002485\n\n\n1995.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n2551727.0\n\n\n14214.441134"
  },
  {
    "objectID": "posts/5_1_2025_SFulton_subdivisions/post.html#data-exploration",
    "href": "posts/5_1_2025_SFulton_subdivisions/post.html#data-exploration",
    "title": "Journal",
    "section": "Data Exploration",
    "text": "Data Exploration\n# Get subdivisions for Fairburn\ndf_fairburn = df[df['City'] == 'Fairburn']\ndf_fairburn.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 65 entries, 226 to 3652\nData columns (total 14 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   OBJECTID       65 non-null     int64  \n 1   SubdivName     65 non-null     object \n 2   CreatedBy      61 non-null     object \n 3   CreateDate     57 non-null     object \n 4   Editor         65 non-null     object \n 5   LastEdit       65 non-null     object \n 6   City           65 non-null     object \n 7   FeatureID      65 non-null     object \n 8   YearRecorded   61 non-null     float64\n 9   DocBook        2 non-null      float64\n 10  DocPage        2 non-null      float64\n 11  DocType        2 non-null      object \n 12  Shape__Area    65 non-null     float64\n 13  Shape__Length  65 non-null     float64\ndtypes: float64(5), int64(1), object(8)\nmemory usage: 7.6+ KB\ndf_fairburn['SubdivName'].unique()\narray(['Park Lane Forest', 'Camden Place', 'Cliftwood',\n       'Fairburn Commons', 'Park Village-Fairburn',\n       'Woodland Hills-Fairburn', 'St. Johns Crossing',\n       'Valley View Estates', 'Legacy At Riverview', 'Coventry Phase 2',\n       'Mary E Stephens', 'Pet Care Properties LLC', 'Southcreek 3 & 4',\n       'Cobblestone Glen Subdivision', 'Jireh Place Subdivision',\n       'Meadow Glen Unit 2', 'Meadow Glen Unit 4', 'Fairhaven Phase 1',\n       'Fairhaven Phase 2', 'Fairhaven Phase 3',\n       'Foxwood Subdivision Phase 2', 'Foxwood Subdivision Phase 3',\n       'Foxwood Subdivision Phase 4', 'Foxwood Subdivision Phase 1',\n       'Milam Manor Phase 1B', 'Milam Manor Phase 2',\n       'Meadow Glen Unit 3', 'Redus One LLC', 'Southpark Phase 3 Unit 1',\n       'Southpark Phase 2', 'The Kirby K Johnson Sr Estate',\n       'Trotters Farm Estates Phase 1', 'Victorian Estates',\n       'Sutton Place', 'Summerwood Unit 1', 'Summerwood Unit 2',\n       'Summerwood Unit 3', 'Shannon Estates',\n       'Trotters Farm Estates Phase 2', 'Trotters Farm Estates Phase 3',\n       'Trotters Farm Estates Phase 4', 'Jonesboro Road LLC',\n       'Avalon Subdivision', 'Olde Campbell Colony Unit 1',\n       'Olde Campbell Colony Unit 2', 'Fairburn Forest Unit 2',\n       'Fairburn Forest Unit 1', 'Rivertown Road Estates Unit 1',\n       'Rivertown Road Estates Unit 3', 'L M Hobgood Estate',\n       'Melanie Condominium', 'Fireside Heights Subdivision',\n       'Williamsburg Subdivision Phase 1', 'Southern Pines - Fairburn',\n       'River Downs Subdivision Phase 1',\n       'Brookhaven at Durham Lakes Unit V Phase I & II',\n       'Durham Lake Unit VI Area 1', 'Durham Lake Unit 1', 'Crofthouse',\n       'Project Miles', 'MD Hodges Enterprises Inc',\n       'The Oaks at Cedar Grove Phase 1', 'Ferndale',\n       'Rivertown Mill Phase 1', 'Rivertown Mill Phase 2'], dtype=object)\n\nAverage Subdivision Shape-Area By City\nsubdivision_shape_area_by_city = df.groupby(\"City\").agg({'Shape__Area':'mean'}).sort_values(by='Shape__Area')\n# Plotting\nplt.figure(figsize=(10, 6))\nsubdivision_shape_area_by_city.plot(kind='bar', legend=False)\nplt.xlabel(\"City\")\nplt.ylabel(\"Average Shape-Area (sq-ft)\")\nplt.title(\"Average Subdivision Shape-Area by City\")\nplt.ticklabel_format(style='plain', axis='y')\nplt.xticks(rotation=90)\nplt.show()\n&lt;Figure size 720x432 with 0 Axes&gt;\n\n\n\npng\n\n\n\n\nAverage Subdivision Shape-Area for the City of Fairburn\ndf_fairburn.groupby(\"SubdivName\").agg({'Shape__Area':'mean'}).sort_values(by='Shape__Area').tail()\n\n\n\n\n\n\n\n\nShape__Area\n\n\n\n\nSubdivName\n\n\n\n\n\n\n\n\nThe Oaks at Cedar Grove Phase 1\n\n\n4045810.0\n\n\n\n\nSouthcreek 3 & 4\n\n\n4229039.0\n\n\n\n\nProject Miles\n\n\n4324595.0\n\n\n\n\nPark Lane Forest\n\n\n4521887.0\n\n\n\n\nMD Hodges Enterprises Inc\n\n\n5802253.0\n\n\n\n\n\ndf_fairburn[['SubdivName','Shape__Area']].tail()\n\n\n\n\n\n\n\n\nSubdivName\n\n\nShape__Area\n\n\n\n\n\n\n3581\n\n\nMD Hodges Enterprises Inc\n\n\n5802253.0\n\n\n\n\n3593\n\n\nThe Oaks at Cedar Grove Phase 1\n\n\n4045810.0\n\n\n\n\n3606\n\n\nFerndale\n\n\n3134399.0\n\n\n\n\n3651\n\n\nRivertown Mill Phase 1\n\n\n2528355.0\n\n\n\n\n3652\n\n\nRivertown Mill Phase 2\n\n\n2109898.0\n\n\n\n\n\nfairburn_subdiv_area = df_fairburn.groupby(\"SubdivName\").agg({'Shape__Area':'mean'}).sort_values(by='Shape__Area').tail()\n# Plotting\nfig, ax = plt.subplots(figsize=(10, 5))\nfairburn_subdiv_area.plot(kind='bar', legend=False, ax=ax)\nax.set_xlabel(\"Subdivision\")\nax.set_ylabel(\"Average Shape Area (square-feet)\")\nax.set_title(\"Fairburn Subdivision Shape-Area\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.ticklabel_format(style='plain', axis='y')\nplt.savefig('top5_fairburn_subdivisions_by_shapearea.png',bbox_inches='tight')\nplt.show()\n\n\n\n\npng\n\n\n\n\nShape-Area over time\ndf_fairburn['YearRecorded'].unique()\narray([1995., 2004., 2007., 2002., 2006., 2017., 1984., 2005., 2008.,\n       2000., 2001., 2003., 1989.,   nan, 1985., 1986., 2018., 1972.,\n       1974., 1971., 1976., 1983., 1979., 1988., 2020., 2021., 2010.])\navg_shape_area_by_year = df_fairburn.groupby(by='YearRecorded').agg({'Shape__Area':'mean'}).sort_values(by='Shape__Area')\navg_shape_area_by_year.plot()\n&lt;AxesSubplot:xlabel='YearRecorded'&gt;\n\n\n\npng"
  },
  {
    "objectID": "posts/5_5_2025_NovaLogin/post.html",
    "href": "posts/5_5_2025_NovaLogin/post.html",
    "title": "Logging Into Nova HPC Cluster",
    "section": "",
    "text": "I‚Äôm tired of looking at these setup/login instructions in my OneNote notebook. It‚Äôs just less messy and more convenient to have everything here."
  },
  {
    "objectID": "posts/5_5_2025_NovaLogin/post.html#step-1-login-to-nova-via-cmd-command--",
    "href": "posts/5_5_2025_NovaLogin/post.html#step-1-login-to-nova-via-cmd-command--",
    "title": "Logging Into Nova HPC Cluster",
    "section": "Step 1: Login to Nova via CMD command ‚Äî‚Äî‚Äî-",
    "text": "Step 1: Login to Nova via CMD command ‚Äî‚Äî‚Äî-\n\nLogin to ISU remote via Cisco Secure Client\nOpen CMD command line and login to Nova\n\n$ ssh my-netid@nova.its.iastate.edu\n\nOpen Google Authenticator to see authentication\nType authentication code in to CMD command line\nType ISU password into CMD command line\nAllocate resources to the Nova session:\n\n$ salloc #desired options"
  },
  {
    "objectID": "posts/5_5_2025_NovaLogin/post.html#step-1-login-to-nova-via-cmd-command-line",
    "href": "posts/5_5_2025_NovaLogin/post.html#step-1-login-to-nova-via-cmd-command-line",
    "title": "Logging Into Nova HPC Cluster",
    "section": "Step 1: Login to Nova via CMD command line üñ•Ô∏è",
    "text": "Step 1: Login to Nova via CMD command line üñ•Ô∏è\n\nLogin to ISU remote via Cisco Secure Client üõ∞Ô∏è\nOpen CMD command line and login to Nova\n\n$ ssh my-netid@nova.its.iastate.edu\n\nOpen Google Authenticator to see authentication\nType authentication code in to CMD command line\nType ISU password into CMD command line\nAllocate resources to the Nova session:\n\n$ salloc #desired options"
  },
  {
    "objectID": "posts/5_5_2025_NovaLogin/post.html#step-2.-initiate-conda-micromamba-environment",
    "href": "posts/5_5_2025_NovaLogin/post.html#step-2.-initiate-conda-micromamba-environment",
    "title": "Logging Into Nova HPC Cluster",
    "section": "Step 2. Initiate Conda (Micromamba) Environment üêç",
    "text": "Step 2. Initiate Conda (Micromamba) Environment üêç\n\nLoad the micromamba module (this is the alternative to using conda Mamba is faster)\n\n$ module load micromamba\n\nIf python environment does not exist (it won‚Äôt if this is your first time logging in), create the environment in the working directory (see working directory above)\n\n$ micromamba create --prefix workingdirectory/env-name/python -c conda-forge\n\nInitialize bash shell to use activate and deactivate\n\n$ eval \"$(micromamba shell hook --shell=bash)\"\n\nActivate the environment using micromamba\n\n$ micromamba activate workingenvironment/env-name"
  },
  {
    "objectID": "posts/5_5_2025_NovaLogin/post.html#step-3.-set-conda-environment-to-work-with-open-on-demand-ood-jupyter-notebook-only-have-to-do-this-once-after-creating-micromamba-environment",
    "href": "posts/5_5_2025_NovaLogin/post.html#step-3.-set-conda-environment-to-work-with-open-on-demand-ood-jupyter-notebook-only-have-to-do-this-once-after-creating-micromamba-environment",
    "title": "Logging Into Nova HPC Cluster",
    "section": "Step 3. Set Conda Environment to Work with Open-On-Demand (OOD) Jupyter Notebook (Only have to do this once after creating Micromamba environment) üììüìíüìî",
    "text": "Step 3. Set Conda Environment to Work with Open-On-Demand (OOD) Jupyter Notebook (Only have to do this once after creating Micromamba environment) üììüìíüìî\nOnce micromamba environment is activated, install ipykernel which is a package that provides the Ipython kernel for jupyter:\n$ micromamba install -c anaconda ipykernel\nWhen ipykernel is installed, the config that OOD needs can be created in the home directory with:\n$ python3 -m ipykernel install --user --name \"env-name\" \nOnce the config is created, the environment can be used by:\n\nLogging in to the appropriate cluster‚Äôs Open OnDemand. See guide here\nCreating a Jupyter Notebook Session\nSelect Interactive Apps\nSelect Jupyter Notebook from the list of apps\nSelect desired compute and partition settings.\nSelect Launch\nOnce it starts, select Connect to Jupyter\nIn the Jupyter Notebook Launcher, select micromamba env-name from the Notebook or Console section. It can also be selected as the kernel when choosing the kernel for a new notebook or console.\n\nAnd that‚Äôs it!!! You‚Äôre connected to the HPC Cluster üòÉüòÉ"
  },
  {
    "objectID": "posts/05_07_2025_DataScienceStudyGuide/post.html",
    "href": "posts/05_07_2025_DataScienceStudyGuide/post.html",
    "title": "My Data Science Study Guide",
    "section": "",
    "text": "Definition: Stochastic Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning models, particularly for linear models like logistic regression and neural networks. It is a variant of gradient descent where the model parameters are updated for each training example, rather than calculating the gradient based on the entire dataset.\nHow it works:\n\nRandomly shuffle the data.\nTake one sample from the training set, compute the gradient of the cost function with respect to the model parameters, and update the parameters using this gradient.\nA cost function (e.g.¬†MSE), or \\(J(\\theta)\\) is defined: \\[\nJ(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})^2\n\\]\n\nwhere:\n\n\\(\\hat{y}^{(i)}\\) is the model‚Äôs prediction for the i-th example,\n\\(y^{(i)}\\) is the actual value for the i-th example,\n\\(\\textit{m}\\) is the total number of examples,\n\\(\\theta\\) represents the parameters of the model.\n\n\nThe gradient is the vector of partial derivatives of the cost function with respect to each model parameter. For each parameter \\(\\theta_j\\), the partial derivative is: \\[\n  \\frac{\\partial J(\\theta)}{\\partial \\theta_j},\n  \\]\n\nThis derivative is computed to update \\(\\theta_j\\).\nHere‚Äôs partial derivative with respect to \\(\\theta_j\\) : \\[,\n  \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}(\\hat{y}^{(i)}-y^{(i)})x_j^{(i)}\n  \\]\nwhere \\(x_j^{(i)}\\) is the value of the \\(i\\)-th example.\n\nThe gradient points in the direction of the steepest ascent of the cost function. To minimize the cost function, we take steps in the opposite direction of the gradient. This leads to a parameter update rule in gradient descent:\n\n\\[\n  \\theta_j := \\theta_j - \\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n  \\]\nwhere \\(\\alpha\\) is the learning rate that controls the step size.\n\nRepeat this for all samples and iterate until convergence.\nRepeat this for all samples and iterate until convergence.\n\nAdvantages:\n\nFaster convergence for large datasets.\nReduces memory usage since updates are done incrementally.\n\nDisadvantages:\n\nCan be noisy, as it updates based on individual data points rather than the full batch.\nMay converge to suboptimal solutions due to noise.\n\n\n\n\n\n\nDefinition: Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, meaning one can be linearly predicted from the others.\nWhy it matters:\n\nIt can lead to inflated standard errors of the coefficients, making it hard to assess the impact of individual predictors.\nCoefficient estimates become unstable and highly sensitive to changes in the model.\n\nDetection Methods:\n\nVariance Inflation Factor (VIF): Measures how much the variance of a regression coefficient is inflated due to multicollinearity.\nCorrelation Matrix: Check for high correlation values between independent variables.\n\nSolutions:\n\nRemove highly correlated predictors.\nUse dimensionality reduction techniques like Principal Component Analysis (PCA).\nUse regularization methods like Ridge or Lasso Regression.\n\n\n\n\n\n\n\n\nType: Supervised learning (regression)\nUse case: Predicting a continuous value (e.g., predicting house prices).\nKey idea: Models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n\n\n\n\n\nType: Supervised learning (classification)\nUse case: Predicting binary outcomes (e.g., spam detection, disease presence).\nKey idea: Uses the logistic function to model binary outcomes, where the output is transformed into probabilities.\n\n\n\n\n\nType: Supervised learning (classification and regression)\nUse case: Modeling decisions with hierarchical structure (e.g., customer segmentation).\nKey idea: Splits data into subsets based on feature values, leading to a tree-like structure of decisions.\n\n\n\n\n\nType: Supervised learning (classification and regression)\nUse case: Robust predictions with high accuracy (e.g., fraud detection, image classification).\nKey idea: An ensemble of decision trees, where multiple trees are built and their predictions averaged (regression) or taken by majority vote (classification).\n\n\n\n\n\nType: Supervised learning (classification)\nUse case: High-dimensional classification problems (e.g., text categorization).\nKey idea: Finds the hyperplane that best separates the classes in the feature space.\n\n\n\n\n\nType: Supervised learning (classification and regression)\nUse case: Simple, instance-based learning (e.g., recommendation systems).\nKey idea: Classifies a new data point based on the majority label of its k-nearest neighbors.\n\n\n\n\n\nType: Unsupervised learning (clustering)\nUse case: Grouping data into clusters (e.g., market segmentation).\nKey idea: Partitions data into k clusters, where each data point belongs to the cluster with the nearest mean.\n\n\n\n\n\nType: Supervised learning (classification and regression)\nUse case: Predictive accuracy in structured data (e.g., financial forecasting).\nKey idea: Builds an ensemble of weak models (e.g., decision trees) in a sequential manner, where each model corrects the errors of its predecessor.\n\n\n\n\n\n\nDefinition: Regularization is a technique to reduce model overfitting by adding a penalty term to the loss function.\nTypes:\n\nRidge Regression (L2 Regularization): Adds a penalty equal to the square of the magnitude of coefficients, shrinking less important features but keeping all in the model.\nLasso Regression (L1 Regularization): Adds a penalty equal to the absolute value of the magnitude of coefficients, leading to some coefficients becoming exactly zero (automatic feature selection).\n\nWhy it matters: Regularization prevents overfitting by discouraging overly complex models that fit the training data too closely but generalize poorly to new data.\n\n\n\n\n\nBias: Error due to overly simplistic assumptions in the model. High bias can lead to underfitting (the model is too simple to capture the underlying patterns).\nVariance: Error due to the model being too sensitive to small fluctuations in the training data. High variance can lead to overfitting (the model is too complex and captures noise as well as the underlying patterns).\nTradeoff: The goal is to balance bias and variance to minimize the total prediction error.\n\n\n\n\n\nDefinition: Cross-validation is a technique used to evaluate the generalization ability of a model by splitting the data into multiple subsets (folds) and training the model on some folds while testing on others.\nTypes:\n\nK-Fold Cross-Validation: Divides the dataset into k equally sized folds, trains the model on k-1 folds, and tests on the remaining fold. This is repeated k times, with each fold used exactly once as the test set.\nLeave-One-Out Cross-Validation (LOOCV): Special case of k-fold cross-validation where k is equal to the number of data points in the dataset.\n\nWhy it matters: Cross-validation helps to assess how well a model will generalize to new, unseen data.\n\n\n\n\n\nDefinition: Techniques used to reduce the number of input variables in a dataset by transforming the data into a lower-dimensional space.\nKey Methods:\n\nPrincipal Component Analysis (PCA): Projects the data onto directions (principal components) that maximize variance.\nt-Distributed Stochastic Neighbor Embedding (t-SNE): A nonlinear technique for dimensionality reduction used mainly for visualization of high-dimensional datasets.\n\nWhy it matters: Reducing the number of dimensions can improve model performance by removing noise, speeding up computations, and avoiding the curse of dimensionality.\n\n\n\n\n\nDefinition: Hyperparameters are parameters that are set before the learning process begins and control the behavior of the model. Tuning refers to the process of finding the best set of hyperparameters for optimal model performance.\nMethods:\n\nGrid Search: Exhaustively tests all combinations of hyperparameter values from a specified set.\nRandom Search: Randomly samples hyperparameter values within a specified range, which can be more efficient for high-dimensional spaces.\nBayesian Optimization: Uses probabilistic models to predict good sets of hyperparameters.\n\n\n\n\n\n\n\n\nPredicted values in binary classification are often probabilities (between 0 and 1) or the final class labels (0 or 1).\nInstead of continuous residuals (differences between predicted and actual values), we deal with probabilities or predicted classes.\n\n\n\n\n\nPurpose: Shows the difference between the predicted probabilities and the actual binary values.\nHow to Use: Calculate the residuals based on the predicted probabilities (not class labels) and plot them against the predicted values.\nDeviance Residuals: These residuals are used to capture the goodness of fit for binary models. If the model predicts a probability of 0.9 for a true event (actual = 1), the residual will be small. If it predicts 0.1 for the same event, the residual will be large.\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import precision_recall_curve, roc_curve, confusion_matrix\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import plot_precision_recall_curve, plot_roc_curve\n# Generate synthetic data for binary classification\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit a Logistic Regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nLogisticRegression()\n# Predict probabilities and class labels\ny_pred_prob = model.predict_proba(X_test)[:, 1]  # Predicted probabilities for class 1\ny_pred = model.predict(X_test)  # Predicted class labels\n\n# 1. Residual Plot for Binary Classification\n# Residuals = Actual - Predicted Probabilities\nresiduals = y_test - y_pred_prob\n\nplt.figure(figsize=(8, 6))\nsns.residplot(x=y_pred_prob, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.xlabel('Predicted Probabilities')\nplt.ylabel('Residuals (Actual - Predicted Probabilities)')\nplt.title('Residual Plot for Binary Classification')\nplt.show()\n\n\n\npng\n\n\nThe red plot in the graphs above represent the LOESS curve. In binary classification, residuals are the differences between the predicted probabilities and the actual labels (0 or 1). When you plot the residuals using a LOESS (Locally Weighted Scatterplot Smoothing) or LOWESS curve, you‚Äôre looking for patterns that can indicate the performance of your model across different ranges of predicted probabilities.\nHere‚Äôs how to interpret the plot:\n\nA flat LOESS curve near zero:\n\nIf the LOESS curve is flat and close to zero, this indicates that your model is well-calibrated. The residuals are evenly distributed across the predicted probabilities, meaning the model is making accurate predictions without any systematic bias. 2. Upward or downward trends:\nIf the curve trends upward or downward, this could indicate bias in the model‚Äôs predictions. Upward trend: Your model is underestimating probabilities for higher predicted values (i.e., predicted probabilities are too low). Downward trend: Your model is overestimating probabilities for higher predicted values (i.e., predicted probabilities are too high). 3. Bowl- or U-shaped curve:\nA U-shaped or bowl-shaped LOESS curve suggests that the model is overconfident in extreme predictions (close to 0 or 1), but less accurate for predictions near 0.5. This is common in binary classification models that tend to overfit or make extreme probability predictions in both classes. 4. S-shaped curve:\nAn S-shaped curve might indicate non-linearity in the relationship between features and the target. The model might need to capture more complex patterns, possibly hinting that you could benefit from a more complex model or feature engineering. 5. Variance around the curve:\nHigh variance or scattered points around the LOESS curve indicate heteroscedasticity (unequal spread of residuals). Ideally, the residuals should be uniformly spread around the LOESS curve, suggesting that the model performs equally well across all predicted probabilities. In summary, you want the LOWESS plot to be as flat and close to zero as possible, indicating minimal bias and good calibration of the model across all ranges of predicted probabilities. Trends or shapes in the plot suggest areas for improvement, such as model calibration or addressing bias.\n\n\n\n\nWhen building machine learning models, it‚Äôs important to evaluate their performance using appropriate metrics. The choice of metric depends on the type of problem (classification, regression, etc.) and the goals of the analysis.\n\n\n\nDefinition: A confusion matrix is a table used to evaluate the performance of a classification model by comparing actual vs.¬†predicted values. It consists of four components:\n\nTrue Positives (TP): Correctly predicted positive instances.\nTrue Negatives (TN): Correctly predicted negative instances.\nFalse Positives (FP): Incorrectly predicted positive instances (Type I error).\nFalse Negatives (FN): Incorrectly predicted negative instances (Type II error).\n\nExample: A confusion matrix for a binary classification model might look like this:\n\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP\nFN\n\n\nActual Negative\nFP\nTN\n\n\n\n\n\n\n\nDefinition: Precision is the ratio of correctly predicted positive observations to the total predicted positives. It answers the question: Of all instances classified as positive, how many were actually positive?\nFormula: \\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\]\nInterpretation: High precision means the model makes fewer false positive predictions. It is important when the cost of false positives is high, such as in medical diagnostics.\n\n\n\n\n\nDefinition: Recall is the ratio of correctly predicted positive observations to all actual positives. It answers the question: Of all actual positive instances, how many were correctly predicted?\nFormula: \\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\]\nInterpretation: High recall indicates that the model captures most of the positive instances, but it might have more false positives. Recall is critical in scenarios where missing a positive case is more costly, such as in disease detection.\n\n\n\n\n\nDefinition: The F1-score is the harmonic mean of precision and recall. It provides a single metric that balances the trade-off between precision and recall.\nFormula: \\[\n\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\nInterpretation: The F1-score is useful when we need a balance between precision and recall, especially in cases where the class distribution is imbalanced.\n\n\n\n\n\nDefinition: Accuracy is the ratio of correctly predicted observations to the total observations.\nFormula: \\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\nInterpretation: Accuracy is a good metric when the classes are balanced, but it can be misleading when the data is imbalanced. For instance, if 95% of instances are negative, a model that predicts everything as negative will have high accuracy but poor precision and recall.\n\n\n\n\n\nDefinition: The ROC curve is a plot of the true positive rate (recall) against the false positive rate (1 - specificity). The area under the curve (AUC) provides a single metric that represents the overall performance of the model.\nInterpretation: AUC-ROC values range from 0.5 (random guessing) to 1.0 (perfect classifier). It is useful for evaluating models on imbalanced datasets because it takes both positive and negative classes into account.\n\n\n\n\n\nDefinition: MSE is used to evaluate regression models. It is the average of the squared differences between actual and predicted values.\nFormula: \\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\nInterpretation: A lower MSE indicates a better model fit. However, since it squares the errors, larger errors are penalized more heavily.\n\n\n\n\n\nDefinition: R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model.\nFormula: \\[\nR^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n\\]\nInterpretation: R¬≤ values range from 0 to 1. A higher R¬≤ value indicates a better fit of the model to the data. An R¬≤ of 0 means the model does not explain any of the variance, while an R¬≤ of 1 means the model explains all the variance."
  },
  {
    "objectID": "posts/05_07_2025_DataScienceStudyGuide/post.html#stochastic-gradient-descent-sgd",
    "href": "posts/05_07_2025_DataScienceStudyGuide/post.html#stochastic-gradient-descent-sgd",
    "title": "My Data Science Study Guide",
    "section": "",
    "text": "Definition: Stochastic Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning models, particularly for linear models like logistic regression and neural networks. It is a variant of gradient descent where the model parameters are updated for each training example, rather than calculating the gradient based on the entire dataset.\nHow it works:\n\nRandomly shuffle the data.\nTake one sample from the training set, compute the gradient of the cost function with respect to the model parameters, and update the parameters using this gradient.\nA cost function (e.g.¬†MSE), or \\(J(\\theta)\\) is defined: \\[\nJ(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})^2\n\\]\n\nwhere:\n\n\\(\\hat{y}^{(i)}\\) is the model‚Äôs prediction for the i-th example,\n\\(y^{(i)}\\) is the actual value for the i-th example,\n\\(\\textit{m}\\) is the total number of examples,\n\\(\\theta\\) represents the parameters of the model.\n\n\nThe gradient is the vector of partial derivatives of the cost function with respect to each model parameter. For each parameter \\(\\theta_j\\), the partial derivative is: \\[\n  \\frac{\\partial J(\\theta)}{\\partial \\theta_j},\n  \\]\n\nThis derivative is computed to update \\(\\theta_j\\).\nHere‚Äôs partial derivative with respect to \\(\\theta_j\\) : \\[,\n  \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}(\\hat{y}^{(i)}-y^{(i)})x_j^{(i)}\n  \\]\nwhere \\(x_j^{(i)}\\) is the value of the \\(i\\)-th example.\n\nThe gradient points in the direction of the steepest ascent of the cost function. To minimize the cost function, we take steps in the opposite direction of the gradient. This leads to a parameter update rule in gradient descent:\n\n\\[\n  \\theta_j := \\theta_j - \\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n  \\]\nwhere \\(\\alpha\\) is the learning rate that controls the step size.\n\nRepeat this for all samples and iterate until convergence.\nRepeat this for all samples and iterate until convergence.\n\nAdvantages:\n\nFaster convergence for large datasets.\nReduces memory usage since updates are done incrementally.\n\nDisadvantages:\n\nCan be noisy, as it updates based on individual data points rather than the full batch.\nMay converge to suboptimal solutions due to noise."
  },
  {
    "objectID": "posts/05_07_2025_DataScienceStudyGuide/post.html#multicollinearity",
    "href": "posts/05_07_2025_DataScienceStudyGuide/post.html#multicollinearity",
    "title": "My Data Science Study Guide",
    "section": "",
    "text": "Definition: Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, meaning one can be linearly predicted from the others.\nWhy it matters:\n\nIt can lead to inflated standard errors of the coefficients, making it hard to assess the impact of individual predictors.\nCoefficient estimates become unstable and highly sensitive to changes in the model.\n\nDetection Methods:\n\nVariance Inflation Factor (VIF): Measures how much the variance of a regression coefficient is inflated due to multicollinearity.\nCorrelation Matrix: Check for high correlation values between independent variables.\n\nSolutions:\n\nRemove highly correlated predictors.\nUse dimensionality reduction techniques like Principal Component Analysis (PCA).\nUse regularization methods like Ridge or Lasso Regression."
  },
  {
    "objectID": "posts/05_07_2025_DataScienceStudyGuide/post.html#common-machine-learning-algorithms",
    "href": "posts/05_07_2025_DataScienceStudyGuide/post.html#common-machine-learning-algorithms",
    "title": "My Data Science Study Guide",
    "section": "",
    "text": "Type: Supervised learning (regression)\nUse case: Predicting a continuous value (e.g., predicting house prices).\nKey idea: Models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n\n\n\n\n\nType: Supervised learning (classification)\nUse case: Predicting binary outcomes (e.g., spam detection, disease presence).\nKey idea: Uses the logistic function to model binary outcomes, where the output is transformed into probabilities.\n\n\n\n\n\nType: Supervised learning (classification and regression)\nUse case: Modeling decisions with hierarchical structure (e.g., customer segmentation).\nKey idea: Splits data into subsets based on feature values, leading to a tree-like structure of decisions.\n\n\n\n\n\nType: Supervised learning (classification and regression)\nUse case: Robust predictions with high accuracy (e.g., fraud detection, image classification).\nKey idea: An ensemble of decision trees, where multiple trees are built and their predictions averaged (regression) or taken by majority vote (classification).\n\n\n\n\n\nType: Supervised learning (classification)\nUse case: High-dimensional classification problems (e.g., text categorization).\nKey idea: Finds the hyperplane that best separates the classes in the feature space.\n\n\n\n\n\nType: Supervised learning (classification and regression)\nUse case: Simple, instance-based learning (e.g., recommendation systems).\nKey idea: Classifies a new data point based on the majority label of its k-nearest neighbors.\n\n\n\n\n\nType: Unsupervised learning (clustering)\nUse case: Grouping data into clusters (e.g., market segmentation).\nKey idea: Partitions data into k clusters, where each data point belongs to the cluster with the nearest mean.\n\n\n\n\n\nType: Supervised learning (classification and regression)\nUse case: Predictive accuracy in structured data (e.g., financial forecasting).\nKey idea: Builds an ensemble of weak models (e.g., decision trees) in a sequential manner, where each model corrects the errors of its predecessor."
  },
  {
    "objectID": "posts/05_07_2025_DataScienceStudyGuide/post.html#regularization",
    "href": "posts/05_07_2025_DataScienceStudyGuide/post.html#regularization",
    "title": "My Data Science Study Guide",
    "section": "",
    "text": "Definition: Regularization is a technique to reduce model overfitting by adding a penalty term to the loss function.\nTypes:\n\nRidge Regression (L2 Regularization): Adds a penalty equal to the square of the magnitude of coefficients, shrinking less important features but keeping all in the model.\nLasso Regression (L1 Regularization): Adds a penalty equal to the absolute value of the magnitude of coefficients, leading to some coefficients becoming exactly zero (automatic feature selection).\n\nWhy it matters: Regularization prevents overfitting by discouraging overly complex models that fit the training data too closely but generalize poorly to new data."
  },
  {
    "objectID": "posts/05_07_2025_DataScienceStudyGuide/post.html#bias-variance-tradeoff",
    "href": "posts/05_07_2025_DataScienceStudyGuide/post.html#bias-variance-tradeoff",
    "title": "My Data Science Study Guide",
    "section": "",
    "text": "Bias: Error due to overly simplistic assumptions in the model. High bias can lead to underfitting (the model is too simple to capture the underlying patterns).\nVariance: Error due to the model being too sensitive to small fluctuations in the training data. High variance can lead to overfitting (the model is too complex and captures noise as well as the underlying patterns).\nTradeoff: The goal is to balance bias and variance to minimize the total prediction error."
  },
  {
    "objectID": "posts/05_07_2025_DataScienceStudyGuide/post.html#cross-validation",
    "href": "posts/05_07_2025_DataScienceStudyGuide/post.html#cross-validation",
    "title": "My Data Science Study Guide",
    "section": "",
    "text": "Definition: Cross-validation is a technique used to evaluate the generalization ability of a model by splitting the data into multiple subsets (folds) and training the model on some folds while testing on others.\nTypes:\n\nK-Fold Cross-Validation: Divides the dataset into k equally sized folds, trains the model on k-1 folds, and tests on the remaining fold. This is repeated k times, with each fold used exactly once as the test set.\nLeave-One-Out Cross-Validation (LOOCV): Special case of k-fold cross-validation where k is equal to the number of data points in the dataset.\n\nWhy it matters: Cross-validation helps to assess how well a model will generalize to new, unseen data."
  },
  {
    "objectID": "posts/05_07_2025_DataScienceStudyGuide/post.html#dimensionality-reduction",
    "href": "posts/05_07_2025_DataScienceStudyGuide/post.html#dimensionality-reduction",
    "title": "My Data Science Study Guide",
    "section": "",
    "text": "Definition: Techniques used to reduce the number of input variables in a dataset by transforming the data into a lower-dimensional space.\nKey Methods:\n\nPrincipal Component Analysis (PCA): Projects the data onto directions (principal components) that maximize variance.\nt-Distributed Stochastic Neighbor Embedding (t-SNE): A nonlinear technique for dimensionality reduction used mainly for visualization of high-dimensional datasets.\n\nWhy it matters: Reducing the number of dimensions can improve model performance by removing noise, speeding up computations, and avoiding the curse of dimensionality."
  },
  {
    "objectID": "posts/05_07_2025_DataScienceStudyGuide/post.html#hyperparameter-tuning",
    "href": "posts/05_07_2025_DataScienceStudyGuide/post.html#hyperparameter-tuning",
    "title": "My Data Science Study Guide",
    "section": "",
    "text": "Definition: Hyperparameters are parameters that are set before the learning process begins and control the behavior of the model. Tuning refers to the process of finding the best set of hyperparameters for optimal model performance.\nMethods:\n\nGrid Search: Exhaustively tests all combinations of hyperparameter values from a specified set.\nRandom Search: Randomly samples hyperparameter values within a specified range, which can be more efficient for high-dimensional spaces.\nBayesian Optimization: Uses probabilistic models to predict good sets of hyperparameters."
  },
  {
    "objectID": "posts/05_07_2025_DataScienceStudyGuide/post.html#assessing-model-performance-for-binary-classification-models",
    "href": "posts/05_07_2025_DataScienceStudyGuide/post.html#assessing-model-performance-for-binary-classification-models",
    "title": "My Data Science Study Guide",
    "section": "",
    "text": "Predicted values in binary classification are often probabilities (between 0 and 1) or the final class labels (0 or 1).\nInstead of continuous residuals (differences between predicted and actual values), we deal with probabilities or predicted classes.\n\n\n\n\n\nPurpose: Shows the difference between the predicted probabilities and the actual binary values.\nHow to Use: Calculate the residuals based on the predicted probabilities (not class labels) and plot them against the predicted values.\nDeviance Residuals: These residuals are used to capture the goodness of fit for binary models. If the model predicts a probability of 0.9 for a true event (actual = 1), the residual will be small. If it predicts 0.1 for the same event, the residual will be large.\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import precision_recall_curve, roc_curve, confusion_matrix\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import plot_precision_recall_curve, plot_roc_curve\n# Generate synthetic data for binary classification\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit a Logistic Regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nLogisticRegression()\n# Predict probabilities and class labels\ny_pred_prob = model.predict_proba(X_test)[:, 1]  # Predicted probabilities for class 1\ny_pred = model.predict(X_test)  # Predicted class labels\n\n# 1. Residual Plot for Binary Classification\n# Residuals = Actual - Predicted Probabilities\nresiduals = y_test - y_pred_prob\n\nplt.figure(figsize=(8, 6))\nsns.residplot(x=y_pred_prob, y=residuals, lowess=True, line_kws={'color': 'red'})\nplt.xlabel('Predicted Probabilities')\nplt.ylabel('Residuals (Actual - Predicted Probabilities)')\nplt.title('Residual Plot for Binary Classification')\nplt.show()\n\n\n\npng\n\n\nThe red plot in the graphs above represent the LOESS curve. In binary classification, residuals are the differences between the predicted probabilities and the actual labels (0 or 1). When you plot the residuals using a LOESS (Locally Weighted Scatterplot Smoothing) or LOWESS curve, you‚Äôre looking for patterns that can indicate the performance of your model across different ranges of predicted probabilities.\nHere‚Äôs how to interpret the plot:\n\nA flat LOESS curve near zero:\n\nIf the LOESS curve is flat and close to zero, this indicates that your model is well-calibrated. The residuals are evenly distributed across the predicted probabilities, meaning the model is making accurate predictions without any systematic bias. 2. Upward or downward trends:\nIf the curve trends upward or downward, this could indicate bias in the model‚Äôs predictions. Upward trend: Your model is underestimating probabilities for higher predicted values (i.e., predicted probabilities are too low). Downward trend: Your model is overestimating probabilities for higher predicted values (i.e., predicted probabilities are too high). 3. Bowl- or U-shaped curve:\nA U-shaped or bowl-shaped LOESS curve suggests that the model is overconfident in extreme predictions (close to 0 or 1), but less accurate for predictions near 0.5. This is common in binary classification models that tend to overfit or make extreme probability predictions in both classes. 4. S-shaped curve:\nAn S-shaped curve might indicate non-linearity in the relationship between features and the target. The model might need to capture more complex patterns, possibly hinting that you could benefit from a more complex model or feature engineering. 5. Variance around the curve:\nHigh variance or scattered points around the LOESS curve indicate heteroscedasticity (unequal spread of residuals). Ideally, the residuals should be uniformly spread around the LOESS curve, suggesting that the model performs equally well across all predicted probabilities. In summary, you want the LOWESS plot to be as flat and close to zero as possible, indicating minimal bias and good calibration of the model across all ranges of predicted probabilities. Trends or shapes in the plot suggest areas for improvement, such as model calibration or addressing bias."
  },
  {
    "objectID": "posts/05_07_2025_DataScienceStudyGuide/post.html#model-performance-evaluation",
    "href": "posts/05_07_2025_DataScienceStudyGuide/post.html#model-performance-evaluation",
    "title": "My Data Science Study Guide",
    "section": "",
    "text": "When building machine learning models, it‚Äôs important to evaluate their performance using appropriate metrics. The choice of metric depends on the type of problem (classification, regression, etc.) and the goals of the analysis.\n\n\n\nDefinition: A confusion matrix is a table used to evaluate the performance of a classification model by comparing actual vs.¬†predicted values. It consists of four components:\n\nTrue Positives (TP): Correctly predicted positive instances.\nTrue Negatives (TN): Correctly predicted negative instances.\nFalse Positives (FP): Incorrectly predicted positive instances (Type I error).\nFalse Negatives (FN): Incorrectly predicted negative instances (Type II error).\n\nExample: A confusion matrix for a binary classification model might look like this:\n\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTP\nFN\n\n\nActual Negative\nFP\nTN\n\n\n\n\n\n\n\nDefinition: Precision is the ratio of correctly predicted positive observations to the total predicted positives. It answers the question: Of all instances classified as positive, how many were actually positive?\nFormula: \\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\]\nInterpretation: High precision means the model makes fewer false positive predictions. It is important when the cost of false positives is high, such as in medical diagnostics.\n\n\n\n\n\nDefinition: Recall is the ratio of correctly predicted positive observations to all actual positives. It answers the question: Of all actual positive instances, how many were correctly predicted?\nFormula: \\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\]\nInterpretation: High recall indicates that the model captures most of the positive instances, but it might have more false positives. Recall is critical in scenarios where missing a positive case is more costly, such as in disease detection.\n\n\n\n\n\nDefinition: The F1-score is the harmonic mean of precision and recall. It provides a single metric that balances the trade-off between precision and recall.\nFormula: \\[\n\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\nInterpretation: The F1-score is useful when we need a balance between precision and recall, especially in cases where the class distribution is imbalanced.\n\n\n\n\n\nDefinition: Accuracy is the ratio of correctly predicted observations to the total observations.\nFormula: \\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\nInterpretation: Accuracy is a good metric when the classes are balanced, but it can be misleading when the data is imbalanced. For instance, if 95% of instances are negative, a model that predicts everything as negative will have high accuracy but poor precision and recall.\n\n\n\n\n\nDefinition: The ROC curve is a plot of the true positive rate (recall) against the false positive rate (1 - specificity). The area under the curve (AUC) provides a single metric that represents the overall performance of the model.\nInterpretation: AUC-ROC values range from 0.5 (random guessing) to 1.0 (perfect classifier). It is useful for evaluating models on imbalanced datasets because it takes both positive and negative classes into account.\n\n\n\n\n\nDefinition: MSE is used to evaluate regression models. It is the average of the squared differences between actual and predicted values.\nFormula: \\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\nInterpretation: A lower MSE indicates a better model fit. However, since it squares the errors, larger errors are penalized more heavily.\n\n\n\n\n\nDefinition: R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model.\nFormula: \\[\nR^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n\\]\nInterpretation: R¬≤ values range from 0 to 1. A higher R¬≤ value indicates a better fit of the model to the data. An R¬≤ of 0 means the model does not explain any of the variance, while an R¬≤ of 1 means the model explains all the variance."
  }
]