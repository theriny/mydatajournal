[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "my data journal",
    "section": "",
    "text": "Stay in touch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogging Into Nova HPC Cluster\n\n\n\npython\n\nmicromamba\n\n\n\nWriting this down to keep future me sane\n\n\n\nTherin Young\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1: Exploring Fulton County’s Subdivision Data\n\n\n\npython\n\nhousing development\n\nrealestate\n\n\n\nExploring subdivision data as it relates to the shape area and length of the subdivisions\n\n\n\nTherin Young\n\n\nMay 1, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Welcome",
    "section": "",
    "text": "👋 Hi, I’m Therin!\nI’m a data scientist passionate about building tools that make data more accessible and actionable. I earned my master’s in mechanical engineering from Iowa State University in 2016, and I’m currently a PhD candidate using machine learning and 3D modeling to build software solutions for precision agriculture applications. I use Python, Azure, and SQL in my current role.\nI spent 2 years at Ford Motor Company as a data scientist on the Material Cost & Analytics team where I used Python, Qlik, and Alteryx to build analytics dashboards and forecasting tools to optimize vehicle feature scheduling and reduce revenue loss. I also spent a year with Collaborative Real Estate gathering insight from FLASH parking data to improve the parker experience.\nWhen I’m not out playing soccer with my two boys or spending time with my wife, I’m producing beats on my Maschine, playing bass guitar, or getting owned by teenagers in Fortnite.\nYou can find some of my work on my Github.\nHere’s a quick animation of how I use 3D modeling and Python to automate the extraction of features (phenotypes) from soybean canopies. https://www.collabre.co/\n\n\nYour browser does not support the video tag. \n\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats GitHub Repo🧑🏾‍💻LinkedIn Article✍🏾"
  },
  {
    "objectID": "projects.html#forecasts",
    "href": "projects.html#forecasts",
    "title": "Projects",
    "section": "",
    "text": "2025 March Madness: A Bayesian forecast of the 2025 men’s and women’s March Madness tournaments (men’s forecast | women’s forecast | repo)"
  },
  {
    "objectID": "projects.html#talks-publications-etc.",
    "href": "projects.html#talks-publications-etc.",
    "title": "Projects",
    "section": "Talks, Publications, etc.",
    "text": "Talks, Publications, etc."
  },
  {
    "objectID": "scholarship.html",
    "href": "scholarship.html",
    "title": "Scholarship",
    "section": "",
    "text": "📚 Publications\nYoung, T.; Chiranjeevi, S.; Elango, D.; Sarkar, S.; Singh, A.K.; Singh, A.; Ganapathysubramanian, B.; Jubery, T.Z.\nSoybean Canopy Stress Classification Using 3D Point Cloud Data. Agronomy 2024, 14, 1181.\nhttps://doi.org/10.3390/agronomy14061181\n\nYoung TJ, Jubery TZ, Carley CN, Carroll M, Sarkar S, Singh AK, Singh A, Ganapathysubramanian B.\n“Canopy fingerprints” for characterizing three-dimensional point cloud data of soybean canopies. Front. Plant Sci. 2023;14:1141153.\nhttps://doi.org/10.3389/fpls.2023.1141153\n\nSingh, A.K., Singh, A., Young, T.\nHigh-Throughput Phenotyping in Soybean. In: High-Throughput Crop Phenotyping. Springer, 2021. pp. 129–163.\nhttps://link.springer.com/chapter/10.1007/978-3-030-82574-1_6\n\nMirnezami, V., Young, T., Assefa, T., Prichard, S., Nagasubramanian, K., Sandhu, K., Sarkar, S., Sundararajan, S., O’Neal, M., Ganapathysubramanian, B., Singh, A.\nTrichome density measurement in soybean leaflet using advanced image processing techniques. Applications in Plant Sciences, 2020; 8(7): e11375.\nhttps://doi.org/10.1002/aps3.11375\n\nYoung, T., Jackson, J., Roy, S., Ceylan, H., Sundararajan, S.\nTribological behavior and wettability of spray-coated superhydrophobic coatings on aluminum. Wear, 2017; 376–377 (Part B): 1713–1719.\nhttps://doi.org/10.1016/j.wear.2016.12.050\n\nYoung, T.\nDevelopment of durable superhydrophobic materials for ice- and snow-free airport concrete pavements. M.S. Thesis, Iowa State University, 2016.\nhttps://dr.lib.iastate.edu/entities/publication/9cf1832e-44cb-4dd5-970b-92a97c0c787b"
  },
  {
    "objectID": "projects.html#sports-ball",
    "href": "projects.html#sports-ball",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats repo"
  },
  {
    "objectID": "projects.html#sport",
    "href": "projects.html#sport",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats repo"
  },
  {
    "objectID": "projects.html#sports",
    "href": "projects.html#sports",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats GitHub Repo🧑🏾‍💻LinkedIn Article✍🏾"
  },
  {
    "objectID": "projects.html#music",
    "href": "projects.html#music",
    "title": "Projects",
    "section": "Music 🎵🎸🎧",
    "text": "Music 🎵🎸🎧\n\nGenius Lyric Scraper🎶: Using Python and Genius’s API tool to scrape song lyrics GitHub Repo🧑🏾‍💻 Thinking of creating a Python library for this one"
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "👋 Hi, I’m Therin!",
    "section": "",
    "text": "👋 Hi, I’m Therin!\nI’m a data scientist passionate about building tools that make data more accessible and actionable. I earned my master’s in mechanical engineering from Iowa State University in 2016, and I’m currently a PhD candidate using machine learning and 3D modeling to build software solutions for precision agriculture applications. I use Python, Azure, and SQL in my current role.\nI spent 2 years at Ford Motor Company as a data scientist on the Material Cost & Analytics team where I used Python, Qlik, and Alteryx to build analytics dashboards and forecasting tools to optimize vehicle feature scheduling and reduce revenue loss. I also spent a year with Collaborative Real Estate gathering insight from FLASH parking data to improve the parker experience.\nWhen I’m not out playing soccer with my two boys or spending time with my wife, I’m producing beats on my Maschine, playing bass guitar, or getting owned by teenagers in Fortnite.\nYou can find some of my work on my Github.\nHere’s a quick animation of how I use 3D modeling and Python to automate the extraction of features (phenotypes) from soybean canopies.\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "scholarship/scholarship.html",
    "href": "scholarship/scholarship.html",
    "title": "Scholarship",
    "section": "",
    "text": "📚 Publications\nYoung, T.; Chiranjeevi, S.; Elango, D.; Sarkar, S.; Singh, A.K.; Singh, A.; Ganapathysubramanian, B.; Jubery, T.Z.\nSoybean Canopy Stress Classification Using 3D Point Cloud Data. Agronomy 2024, 14, 1181.\nhttps://doi.org/10.3390/agronomy14061181\n\nYoung TJ, Jubery TZ, Carley CN, Carroll M, Sarkar S, Singh AK, Singh A, Ganapathysubramanian B.\n“Canopy fingerprints” for characterizing three-dimensional point cloud data of soybean canopies. Front. Plant Sci. 2023;14:1141153.\nhttps://doi.org/10.3389/fpls.2023.1141153\n\nSingh, A.K., Singh, A., Young, T.\nHigh-Throughput Phenotyping in Soybean. In: High-Throughput Crop Phenotyping. Springer, 2021. pp. 129–163.\nhttps://link.springer.com/chapter/10.1007/978-3-030-82574-1_6\n\nMirnezami, V., Young, T., Assefa, T., Prichard, S., Nagasubramanian, K., Sandhu, K., Sarkar, S., Sundararajan, S., O’Neal, M., Ganapathysubramanian, B., Singh, A.\nTrichome density measurement in soybean leaflet using advanced image processing techniques. Applications in Plant Sciences, 2020; 8(7): e11375.\nhttps://doi.org/10.1002/aps3.11375\n\nYoung, T., Jackson, J., Roy, S., Ceylan, H., Sundararajan, S.\nTribological behavior and wettability of spray-coated superhydrophobic coatings on aluminum. Wear, 2017; 376–377 (Part B): 1713–1719.\nhttps://doi.org/10.1016/j.wear.2016.12.050\n\nYoung, T.\nDevelopment of durable superhydrophobic materials for ice- and snow-free airport concrete pavements. M.S. Thesis, Iowa State University, 2016.\nhttps://dr.lib.iastate.edu/entities/publication/9cf1832e-44cb-4dd5-970b-92a97c0c787b"
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats GitHub Repo🧑🏾‍💻LinkedIn Article✍🏾"
  },
  {
    "objectID": "projects/projects.html#sports",
    "href": "projects/projects.html#sports",
    "title": "Projects",
    "section": "",
    "text": "Predicting NBA Game Using Defensive Stats: A machine learning classsificatin model trained with NBA defensive stats GitHub Repo🧑🏾‍💻LinkedIn Article✍🏾"
  },
  {
    "objectID": "projects/projects.html#music",
    "href": "projects/projects.html#music",
    "title": "Projects",
    "section": "Music 🎵🎸🎧",
    "text": "Music 🎵🎸🎧\n\nGenius Lyric Scraper🎶: Using Python and Genius’s API tool to scrape song lyrics GitHub Repo🧑🏾‍💻 Thinking of creating a Python library for this one"
  },
  {
    "objectID": "posts/5_1_2025_SFulton_subdivisions/post.html",
    "href": "posts/5_1_2025_SFulton_subdivisions/post.html",
    "title": "Part 1: Exploring Fulton County’s Subdivision Data",
    "section": "",
    "text": "This is part 1 of a 2- (maybe 3) series analysis of Fulton County, Georgia’s subdivisions, where the only metrics recorded are shape area and shape length📐📏 (A little more detail about those metrics later in the analysis). Doesn’t seem like a very interesting dataset at first glance🥸, but I always uncover interesting and unexpected nuggets🪙 of information from seemingly dull datasets. So, if I find a rabbit hole, let’s hope it’s a fun one.🐰\nI’m going to start the first part of the series from far out🛰️ (basic statistics) and gradually hone in for a more nuanced analysis🔬 as the series progresses and I begin to ask questions about the data that is relevant to my own location in South Fulton.\nI’m going to place this series in Housing and Development🏘️, so readers who are experts or simply enthusiasts of the field can feel free to offer suggestions, comments, and any version of personal opinion on the matter. I’d love to hear other takes and approaches to extracting meaning from this type of dataset.\nHappy Exploring! 🏔️\nData Source: https://gisdata.fultoncountyga.gov/ GitHub Repo: https://github.com/theriny/fultoncounty_subdivisions\n\nGet Dependencies\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n\nRead Data\ndf = pd.read_csv('Subdivisions.csv')\ndf.describe()\n\n\n\n\n\n\n\n\nOBJECTID\n\n\nYearRecorded\n\n\nDocBook\n\n\nDocPage\n\n\nShape__Area\n\n\nShape__Length\n\n\n\n\n\n\ncount\n\n\n3677.000000\n\n\n1823.000000\n\n\n83.000000\n\n\n83.000000\n\n\n3.675000e+03\n\n\n3675.000000\n\n\n\n\nmean\n\n\n2081.452543\n\n\n2002.259462\n\n\n313.493976\n\n\n79.000000\n\n\n1.107878e+06\n\n\n4214.630614\n\n\n\n\nstd\n\n\n1918.580076\n\n\n26.901355\n\n\n131.934978\n\n\n41.136596\n\n\n2.252761e+06\n\n\n3748.502907\n\n\n\n\nmin\n\n\n1.000000\n\n\n978.000000\n\n\n99.000000\n\n\n1.000000\n\n\n3.198242e-02\n\n\n16.750267\n\n\n\n\n25%\n\n\n941.000000\n\n\n1995.000000\n\n\n183.500000\n\n\n44.000000\n\n\n1.702545e+05\n\n\n1867.852593\n\n\n\n\n50%\n\n\n1876.000000\n\n\n2004.000000\n\n\n333.000000\n\n\n75.000000\n\n\n5.075356e+05\n\n\n3355.910444\n\n\n\n\n75%\n\n\n2799.000000\n\n\n2013.000000\n\n\n445.500000\n\n\n114.500000\n\n\n1.218273e+06\n\n\n5457.816465\n\n\n\n\nmax\n\n\n14872.000000\n\n\n2023.000000\n\n\n464.000000\n\n\n148.000000\n\n\n4.476438e+07\n\n\n55177.479838\n\n\n\n\n\nThis dataset contains the shape area and shape length of subdivisions for various cities in Fulton County, Georgia that span from 978 to 2023. 978!?!?😲 It seems we may have a typo. Let’s look at the unique years that were recorded.\n\n\nA little tidying up\nYea, let’s change 978 to 1978. How many times was this typo made?\ndf[df['YearRecorded'] == 978.]\n\n\n\n\n\n\n\n\nOBJECTID\n\n\nSubdivName\n\n\nCreatedBy\n\n\nCreateDate\n\n\nEditor\n\n\nLastEdit\n\n\nCity\n\n\nFeatureID\n\n\nYearRecorded\n\n\nDocBook\n\n\nDocPage\n\n\nDocType\n\n\nShape__Area\n\n\nShape__Length\n\n\n\n\n\n\n3642\n\n\n13636\n\n\nCameron Crest Farms\n\n\nSTEPHANIE.YANCEY\n\n\n2023/10/17 12:07:11+00\n\n\nSTEPHANIE.YANCEY\n\n\n2023/10/17 12:08:24+00\n\n\nJohns Creek\n\n\nLSD0000413\n\n\n978.0\n\n\n113.0\n\n\n4.0\n\n\nPL\n\n\n5.919767e+06\n\n\n11584.65774\n\n\n\n\n\nOnly once on row 3642. So let’s make a quick fix using pandas .loc function.\n# replace 978 in row 3642 with 1978\ndf.loc[3642,'YearRecorded'] = 1978\n#sort record dates\nnp.sort(df['YearRecorded'].unique())\narray([1939., 1961., 1968., 1970., 1971., 1972., 1973., 1974., 1975.,\n       1976., 1977., 1978., 1979., 1980., 1981., 1982., 1983., 1984.,\n       1985., 1986., 1987., 1988., 1989., 1990., 1991., 1992., 1993.,\n       1994., 1995., 1996., 1997., 1998., 1999., 2000., 2001., 2002.,\n       2003., 2004., 2005., 2006., 2007., 2008., 2009., 2010., 2011.,\n       2012., 2013., 2014., 2015., 2016., 2017., 2018., 2019., 2020.,\n       2021., 2022., 2023.,   nan])\nSince the YearRecorded field will be used later down the line, I want to remove any rows with nan in this column. How many of these rows are there?\n(df[np.isnan(df['YearRecorded']) == True].shape[0])/df.shape[0]\n0.5042153929834103\ndf.shape\n(3677, 14)\nThat’s 50% of the dataset 😲. Now, there are two columns in the dataset: CreateDate and LastEdit, which have the same date value and seem to be dates related to when the editor of the dataset, Stephanie Yancey (Column: CreatedBy) edited or entered the data, so not necessarily the original date of when the information was first recorded. I could replace the nan dates of YearRecorded with one these dates, but that could lead to faulty results when I want to look at things like what was the average subdivision shape area in 2023?, since the majority of Stephanie Yancey’s edits were in 2023.\nSooo, I’m going to remove the nan rows (for now).\n# remove rows where YearRecorded is nan\ndf = df[np.isnan(df['YearRecorded']) != True]\n# confirm rows were removed\ndf.shape\n(1823, 14)\nLet’s also convert shape area from square-feet to acres. I’m guessing the current values for shape area are in square feet, and shape length is in feet. 📏📐\n# convert square feet to acres (43,560 square feet is 1 acre)\ndf['Shape__Area'] = df['Shape__Area'].apply(lambda x: x/43560.0)\n\n\nExploration 🧭🔍\nAwesome!🙆🏾 So, the subdivision shape area and length data recordings range from 1939 to 2023. I’m curious to know if the area and/or length of a subdivision can change over time. I’m sure it can if say, a subdivision is rezoned due to home reductions or additions, but I want to know if those events are captured in the data. I’ll create a function that will essentially: 1. Determine if a subdivision name is in more than one row of the dataset.💭(duplicate entry OR same subdiv name but different city OR same subdiv name and city but different shape area/length and record dates (the gold 🪙) 2. Determine if the subdivision’s city is the same for each additional appearance of the subdivision (if it’s not, this probably means the cities have subdivisions with the same name, which is common) 💭 3. The function will return a table that lists all subdivisions that meet the above criteria.💭\n# 🧑🏾‍💻Create function that finds subdivisions with more than 1 record where the measurement and/or year are different between the records\n\ndef Subdivisions_With_Updates(dataframe):\n    \n    \n    # create empty lists to capture city, year, shape, subdivname\n    city = []\n    year = []\n    shape = []\n    subdivname = []\n    \n    \n    for subdiv in dataframe['SubdivName'].unique():\n        \n        # get subset of dataframe only containing current subdivision\n        uniqueSubs = dataframe[dataframe['SubdivName'] == subdiv]\n        \n        # get the unique cities associated with current subdivision\n        uniqueCity = dataframe[dataframe['SubdivName'] == subdiv]['City'].unique()\n        \n        if (len(uniqueSubs) &gt; 1) & (len(uniqueCity) == 1): #if the subdivision appears more than once and there is one unique city, this indicates a change in the subdivisions recorded area/length\n            for i in range(0,len(uniqueSubs)):\n                uniqueSubs = uniqueSubs.reset_index(drop=True) # reset index\n                subdivname.append(uniqueSubs['SubdivName'][i])\n                city.append(uniqueSubs['City'][i])\n                year.append(uniqueSubs['YearRecorded'][i])\n                shape.append(uniqueSubs['Shape__Area'][i])\n            \n        else:\n            None\n    results = pd.DataFrame({'SubDivName': subdivname, 'City': city, 'Year': year, 'Shape Area': shape})\n    \n    return results      \n# Run the function and save results to 'results' variable\nresults = Subdivisions_With_Updates(df)\nresults.head()\n\n\n\n\n\n\n\n\nSubDivName\n\n\nCity\n\n\nYear\n\n\nShape Area\n\n\n\n\n\n\n0\n\n\nDPMS Builders LLC Phase 3\n\n\nAtlanta\n\n\n2017.0\n\n\n0.246343\n\n\n\n\n1\n\n\nDPMS Builders LLC Phase 3\n\n\nAtlanta\n\n\n2016.0\n\n\n1.868496\n\n\n\n\n2\n\n\nOverton Hills\n\n\nSandy Springs\n\n\n1978.0\n\n\n17.816186\n\n\n\n\n3\n\n\nOverton Hills\n\n\nSandy Springs\n\n\n1978.0\n\n\n15.884354\n\n\n\n\n4\n\n\nCameron Crest Farms\n\n\nJohns Creek\n\n\n1978.0\n\n\n219.386157\n\n\n\n\n\n# get unique subdivisions whose shape area has at some point in time\nlen(list(results['SubDivName'].unique()))\n15\n# get unique subdivisions in dataset\ndf.groupby(by=['SubdivName','City']).ngroups\n1791\nlen(list(df['SubdivName'].unique()))\n1803\nWe can see 🙈 that 21 of the subdivisions’ shape size has changed at some point in time. Thats 21 out of 3,628 unique subdivisions in the dataset (taking into account that some cities share subdivivision names)…That’s less than 1%🤏🏾. This indicates that subdivisions are rarely reduced or increased in size after they have been developed (in Fulton County, GA).\nI’m also curious to know if the average shape area of subdivisions has evolved over the years.\n# calculate the average subdivision shape area for each year\nshapeArea_perYear = df.groupby(by=['YearRecorded']).agg({'Shape__Area': 'mean'})\nplt.bar(shapeArea_perYear.index,shapeArea_perYear.Shape__Area)\nplt.ylabel('Shape Area (acres)')\nplt.xlabel('YearRecorded')\nplt.show()\nText(0.5, 0, 'YearRecorded')\n\n\n\npng\n\n\nIt’s interesting to note that there was a 20-year gap between 1940 and 1960 where no data was reported. 😯\nWhat subdivisions existed in 1939 in Fulton County, GA???\ndf[df['YearRecorded'] == 1939.0]\n\n\n\n\n\n\n\n\nOBJECTID\n\n\nSubdivName\n\n\nCreatedBy\n\n\nCreateDate\n\n\nEditor\n\n\nLastEdit\n\n\nCity\n\n\nFeatureID\n\n\nYearRecorded\n\n\nDocBook\n\n\nDocPage\n\n\nDocType\n\n\nShape__Area\n\n\nShape__Length\n\n\n\n\n\n\n2923\n\n\n2965\n\n\nBroad Subdivision\n\n\nCHRIS.MCMILLER\n\n\n2019/03/28 14:36:37+00\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nSouth Fulton\n\n\nLSD0000329\n\n\n1939.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n200.350853\n\n\n20924.700663\n\n\n\n\n\nBroad Subdivision🤔 I’m guessing this is a collection of subdivisions rolled into a single datapoint because it’s just weird to think a single subdivision was that large in 1939 and non-existent 20 years later after no data was recorded for 20 years. It’s fair to say data recording didn’t become more consistent until the 1970’s, so it might be fair cut datapoints earlier than 1970 moving forward. There are, however, additional peaks in shape area for 4 or 5 of the years after 1970 that should be investigated.\nWhat about subdivision shape length?\n#calculate average subdivision shape length per year\nshapeLength_perYear = df.groupby(by='YearRecorded').agg({'Shape__Length':'mean'})\n\nplt.bar(shapeLength_perYear.index,shapeLength_perYear.Shape__Length)\nplt.xlabel('YearRecorded')\nplt.ylabel('Shape Length (ft)')\nplt.show()\nText(0, 0.5, 'Shape Length (ft)')\n\n\n\npng\n\n\nLet’s end the first part of the series by looking at the top 5️⃣ subdivisions with the largest shape areas. But lets first remove any recordings before 1970.\n# get records from 1970 and up\ndf_post1970 = df[df['YearRecorded'] &gt; 1969.0]\nsorted_post1970 = df_post1970.sort_values(by='Shape__Area',ascending=False)\nsorted_post1970.head()\n\n\n\n\n\n\n\n\nOBJECTID\n\n\nSubdivName\n\n\nCreatedBy\n\n\nCreateDate\n\n\nEditor\n\n\nLastEdit\n\n\nCity\n\n\nFeatureID\n\n\nYearRecorded\n\n\nDocBook\n\n\nDocPage\n\n\nDocType\n\n\nShape__Area\n\n\nShape__Length\n\n\n\n\n\n\n518\n\n\n539\n\n\nMartins Landing/Lakeview Homes\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nRoswell\n\n\nLSD0001546\n\n\n1981.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n1027.648744\n\n\n34842.968347\n\n\n\n\n3373\n\n\n3415\n\n\nHorseshoe Bend\n\n\nDALU.FAB-UKOZOR\n\n\n2020/07/30 14:08:32+00\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nNaN\n\n\nLSD0001239\n\n\n1994.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n860.696880\n\n\n40867.862487\n\n\n\n\n461\n\n\n482\n\n\nWillow Springs\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nRoswell\n\n\nLSD0002852\n\n\n1985.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n581.907403\n\n\n26407.221626\n\n\n\n\n668\n\n\n689\n\n\nSaddle Creek\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nRoswell\n\n\nLSD0002204\n\n\n1985.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n431.206870\n\n\n18912.245659\n\n\n\n\n2640\n\n\n2682\n\n\nCedar Grove Village Phase 1 Section A\n\n\nCHRIS.MCMILLER\n\n\n2018/12/20 12:25:46+00\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:54:34+00\n\n\nSouth Fulton\n\n\nLSD0000483\n\n\n2003.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n377.121148\n\n\n21131.509310\n\n\n\n\n\nsns.barplot(data=sorted_post1970.iloc[:5], x = 'SubdivName',y= 'Shape__Area')\nplt.ylabel('Shape Area (acres)')\nplt.xlabel('Subdivision')\nplt.xticks(rotation=90)\nplt.show()\n(array([0, 1, 2, 3, 4]),\n [Text(0, 0, 'Martins Landing/Lakeview Homes'),\n  Text(1, 0, 'Horseshoe Bend'),\n  Text(2, 0, 'Willow Springs'),\n  Text(3, 0, 'Saddle Creek'),\n  Text(4, 0, 'Cedar Grove Village Phase 1 Section A')])\n\n\n\npng\n\n\nAnd, that is an interesting place to end. The 5th largest subdivision shape area happens to be the subdivision I live in, at about 400 acres! 😲🙂\nThe next installment to this series is coming soon. Until then, Happy Exploring.\n\n\n\n\nCitationBibTeX citation:@online{young2025,\n  author = {Young, Therin},\n  title = {Part 1: {Exploring} {Fulton} {County’s} {Subdivision} {Data}},\n  date = {2025-05-01},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nYoung, Therin. 2025. “Part 1: Exploring Fulton County’s\nSubdivision Data.” May 1, 2025."
  },
  {
    "objectID": "posts/5_1_2025_SFulton_subdivisions/post.html#data-cleaningprocessing",
    "href": "posts/5_1_2025_SFulton_subdivisions/post.html#data-cleaningprocessing",
    "title": "Journal",
    "section": "Data Cleaning/Processing",
    "text": "Data Cleaning/Processing\n#convert shape_area fild to integer values\ndf['Shape__Area'] = df['Shape__Area'].apply(lambda x: int(x) if not pd.isna(x) else x)\ndf.head()\n\n\n\n\n\n\n\n\nOBJECTID\n\n\nSubdivName\n\n\nCreatedBy\n\n\nCreateDate\n\n\nEditor\n\n\nLastEdit\n\n\nCity\n\n\nFeatureID\n\n\nYearRecorded\n\n\nDocBook\n\n\nDocPage\n\n\nDocType\n\n\nShape__Area\n\n\nShape__Length\n\n\n\n\n\n\n0\n\n\n1\n\n\nPeachtree Gardens\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nAtlanta\n\n\nLSD0001908\n\n\n1995.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n694869.0\n\n\n5831.032211\n\n\n\n\n1\n\n\n2\n\n\nSpringlake Park/spring Lake\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nAtlanta\n\n\nLSD0002333\n\n\n1995.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n427291.0\n\n\n2957.645702\n\n\n\n\n2\n\n\n3\n\n\nAwesome Homes Inc\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nAtlanta\n\n\nLSD0000188\n\n\n1995.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n184450.0\n\n\n1855.649760\n\n\n\n\n3\n\n\n4\n\n\nColumbia Residential of Mechanicsville Residen…\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:22:32+00\n\n\nAtlanta\n\n\nLSD0000590\n\n\n2015.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n140338.0\n\n\n2290.047939\n\n\n\n\n4\n\n\n5\n\n\nThe Central Buckhead/twnhs\n\n\nNaN\n\n\nNaN\n\n\nSTEPHANIE.YANCEY\n\n\n2023/09/28 10:44:07+00\n\n\nAtlanta\n\n\nLSD0002485\n\n\n1995.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n2551727.0\n\n\n14214.441134"
  },
  {
    "objectID": "posts/5_1_2025_SFulton_subdivisions/post.html#data-exploration",
    "href": "posts/5_1_2025_SFulton_subdivisions/post.html#data-exploration",
    "title": "Journal",
    "section": "Data Exploration",
    "text": "Data Exploration\n# Get subdivisions for Fairburn\ndf_fairburn = df[df['City'] == 'Fairburn']\ndf_fairburn.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 65 entries, 226 to 3652\nData columns (total 14 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   OBJECTID       65 non-null     int64  \n 1   SubdivName     65 non-null     object \n 2   CreatedBy      61 non-null     object \n 3   CreateDate     57 non-null     object \n 4   Editor         65 non-null     object \n 5   LastEdit       65 non-null     object \n 6   City           65 non-null     object \n 7   FeatureID      65 non-null     object \n 8   YearRecorded   61 non-null     float64\n 9   DocBook        2 non-null      float64\n 10  DocPage        2 non-null      float64\n 11  DocType        2 non-null      object \n 12  Shape__Area    65 non-null     float64\n 13  Shape__Length  65 non-null     float64\ndtypes: float64(5), int64(1), object(8)\nmemory usage: 7.6+ KB\ndf_fairburn['SubdivName'].unique()\narray(['Park Lane Forest', 'Camden Place', 'Cliftwood',\n       'Fairburn Commons', 'Park Village-Fairburn',\n       'Woodland Hills-Fairburn', 'St. Johns Crossing',\n       'Valley View Estates', 'Legacy At Riverview', 'Coventry Phase 2',\n       'Mary E Stephens', 'Pet Care Properties LLC', 'Southcreek 3 & 4',\n       'Cobblestone Glen Subdivision', 'Jireh Place Subdivision',\n       'Meadow Glen Unit 2', 'Meadow Glen Unit 4', 'Fairhaven Phase 1',\n       'Fairhaven Phase 2', 'Fairhaven Phase 3',\n       'Foxwood Subdivision Phase 2', 'Foxwood Subdivision Phase 3',\n       'Foxwood Subdivision Phase 4', 'Foxwood Subdivision Phase 1',\n       'Milam Manor Phase 1B', 'Milam Manor Phase 2',\n       'Meadow Glen Unit 3', 'Redus One LLC', 'Southpark Phase 3 Unit 1',\n       'Southpark Phase 2', 'The Kirby K Johnson Sr Estate',\n       'Trotters Farm Estates Phase 1', 'Victorian Estates',\n       'Sutton Place', 'Summerwood Unit 1', 'Summerwood Unit 2',\n       'Summerwood Unit 3', 'Shannon Estates',\n       'Trotters Farm Estates Phase 2', 'Trotters Farm Estates Phase 3',\n       'Trotters Farm Estates Phase 4', 'Jonesboro Road LLC',\n       'Avalon Subdivision', 'Olde Campbell Colony Unit 1',\n       'Olde Campbell Colony Unit 2', 'Fairburn Forest Unit 2',\n       'Fairburn Forest Unit 1', 'Rivertown Road Estates Unit 1',\n       'Rivertown Road Estates Unit 3', 'L M Hobgood Estate',\n       'Melanie Condominium', 'Fireside Heights Subdivision',\n       'Williamsburg Subdivision Phase 1', 'Southern Pines - Fairburn',\n       'River Downs Subdivision Phase 1',\n       'Brookhaven at Durham Lakes Unit V Phase I & II',\n       'Durham Lake Unit VI Area 1', 'Durham Lake Unit 1', 'Crofthouse',\n       'Project Miles', 'MD Hodges Enterprises Inc',\n       'The Oaks at Cedar Grove Phase 1', 'Ferndale',\n       'Rivertown Mill Phase 1', 'Rivertown Mill Phase 2'], dtype=object)\n\nAverage Subdivision Shape-Area By City\nsubdivision_shape_area_by_city = df.groupby(\"City\").agg({'Shape__Area':'mean'}).sort_values(by='Shape__Area')\n# Plotting\nplt.figure(figsize=(10, 6))\nsubdivision_shape_area_by_city.plot(kind='bar', legend=False)\nplt.xlabel(\"City\")\nplt.ylabel(\"Average Shape-Area (sq-ft)\")\nplt.title(\"Average Subdivision Shape-Area by City\")\nplt.ticklabel_format(style='plain', axis='y')\nplt.xticks(rotation=90)\nplt.show()\n&lt;Figure size 720x432 with 0 Axes&gt;\n\n\n\npng\n\n\n\n\nAverage Subdivision Shape-Area for the City of Fairburn\ndf_fairburn.groupby(\"SubdivName\").agg({'Shape__Area':'mean'}).sort_values(by='Shape__Area').tail()\n\n\n\n\n\n\n\n\nShape__Area\n\n\n\n\nSubdivName\n\n\n\n\n\n\n\n\nThe Oaks at Cedar Grove Phase 1\n\n\n4045810.0\n\n\n\n\nSouthcreek 3 & 4\n\n\n4229039.0\n\n\n\n\nProject Miles\n\n\n4324595.0\n\n\n\n\nPark Lane Forest\n\n\n4521887.0\n\n\n\n\nMD Hodges Enterprises Inc\n\n\n5802253.0\n\n\n\n\n\ndf_fairburn[['SubdivName','Shape__Area']].tail()\n\n\n\n\n\n\n\n\nSubdivName\n\n\nShape__Area\n\n\n\n\n\n\n3581\n\n\nMD Hodges Enterprises Inc\n\n\n5802253.0\n\n\n\n\n3593\n\n\nThe Oaks at Cedar Grove Phase 1\n\n\n4045810.0\n\n\n\n\n3606\n\n\nFerndale\n\n\n3134399.0\n\n\n\n\n3651\n\n\nRivertown Mill Phase 1\n\n\n2528355.0\n\n\n\n\n3652\n\n\nRivertown Mill Phase 2\n\n\n2109898.0\n\n\n\n\n\nfairburn_subdiv_area = df_fairburn.groupby(\"SubdivName\").agg({'Shape__Area':'mean'}).sort_values(by='Shape__Area').tail()\n# Plotting\nfig, ax = plt.subplots(figsize=(10, 5))\nfairburn_subdiv_area.plot(kind='bar', legend=False, ax=ax)\nax.set_xlabel(\"Subdivision\")\nax.set_ylabel(\"Average Shape Area (square-feet)\")\nax.set_title(\"Fairburn Subdivision Shape-Area\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.ticklabel_format(style='plain', axis='y')\nplt.savefig('top5_fairburn_subdivisions_by_shapearea.png',bbox_inches='tight')\nplt.show()\n\n\n\n\npng\n\n\n\n\nShape-Area over time\ndf_fairburn['YearRecorded'].unique()\narray([1995., 2004., 2007., 2002., 2006., 2017., 1984., 2005., 2008.,\n       2000., 2001., 2003., 1989.,   nan, 1985., 1986., 2018., 1972.,\n       1974., 1971., 1976., 1983., 1979., 1988., 2020., 2021., 2010.])\navg_shape_area_by_year = df_fairburn.groupby(by='YearRecorded').agg({'Shape__Area':'mean'}).sort_values(by='Shape__Area')\navg_shape_area_by_year.plot()\n&lt;AxesSubplot:xlabel='YearRecorded'&gt;\n\n\n\npng"
  },
  {
    "objectID": "posts/5_5_2025_NovaLogin/post.html",
    "href": "posts/5_5_2025_NovaLogin/post.html",
    "title": "Logging Into Nova HPC Cluster",
    "section": "",
    "text": "I’m tired of looking at these setup/login instructions in my OneNote notebook. It’s just less messy and more convenient to have everything here."
  },
  {
    "objectID": "posts/5_5_2025_NovaLogin/post.html#step-1-login-to-nova-via-cmd-command--",
    "href": "posts/5_5_2025_NovaLogin/post.html#step-1-login-to-nova-via-cmd-command--",
    "title": "Logging Into Nova HPC Cluster",
    "section": "Step 1: Login to Nova via CMD command ———-",
    "text": "Step 1: Login to Nova via CMD command ———-\n\nLogin to ISU remote via Cisco Secure Client\nOpen CMD command line and login to Nova\n\n$ ssh my-netid@nova.its.iastate.edu\n\nOpen Google Authenticator to see authentication\nType authentication code in to CMD command line\nType ISU password into CMD command line\nAllocate resources to the Nova session:\n\n$ salloc #desired options"
  },
  {
    "objectID": "posts/5_5_2025_NovaLogin/post.html#step-1-login-to-nova-via-cmd-command-line",
    "href": "posts/5_5_2025_NovaLogin/post.html#step-1-login-to-nova-via-cmd-command-line",
    "title": "Logging Into Nova HPC Cluster",
    "section": "Step 1: Login to Nova via CMD command line 🖥️",
    "text": "Step 1: Login to Nova via CMD command line 🖥️\n\nLogin to ISU remote via Cisco Secure Client 🛰️\nOpen CMD command line and login to Nova\n\n$ ssh my-netid@nova.its.iastate.edu\n\nOpen Google Authenticator to see authentication\nType authentication code in to CMD command line\nType ISU password into CMD command line\nAllocate resources to the Nova session:\n\n$ salloc #desired options"
  },
  {
    "objectID": "posts/5_5_2025_NovaLogin/post.html#step-2.-initiate-conda-micromamba-environment",
    "href": "posts/5_5_2025_NovaLogin/post.html#step-2.-initiate-conda-micromamba-environment",
    "title": "Logging Into Nova HPC Cluster",
    "section": "Step 2. Initiate Conda (Micromamba) Environment 🐍",
    "text": "Step 2. Initiate Conda (Micromamba) Environment 🐍\n\nLoad the micromamba module (this is the alternative to using conda Mamba is faster)\nIf python environment does not exist (it won’t if this is your first time logging in), create the environment in the working directory (see working directory above)\n\n$ micromamba create --prefix workingdirectory/env-name/python -c conda-forge\n\nInitialize bash shell to use activate and deactivate\n\n$ eval \"$(micromamba shell hook --shell=bash)\"\n\nActivate the environment using micromamba\n\n$ micromamba activate workingenvironment/env-name"
  },
  {
    "objectID": "posts/5_5_2025_NovaLogin/post.html#step-3.-set-conda-environment-to-work-with-open-on-demand-ood-jupyter-notebook-only-have-to-do-this-once-after-creating-micromamba-environment",
    "href": "posts/5_5_2025_NovaLogin/post.html#step-3.-set-conda-environment-to-work-with-open-on-demand-ood-jupyter-notebook-only-have-to-do-this-once-after-creating-micromamba-environment",
    "title": "Logging Into Nova HPC Cluster",
    "section": "Step 3. Set Conda Environment to Work with Open-On-Demand (OOD) Jupyter Notebook (Only have to do this once after creating Micromamba environment) 📓📒📔",
    "text": "Step 3. Set Conda Environment to Work with Open-On-Demand (OOD) Jupyter Notebook (Only have to do this once after creating Micromamba environment) 📓📒📔\nOnce micromamba environment is activated, install ipykernel which is a package that provides the Ipython kernel for jupyter:\n$ micromamba install -c anaconda ipykernel\nWhen ipykernel is installed, the config that OOD nees can be created in the home directory with:\n$ python3 -m ipykernel install --user --name \"env-name\" # ONLY RUN ONCE\nOnce the config is created, the environment can be used by:\n\nLogging in to the appropriate cluster’s Open OnDemand. See guide here\nCreating a Jupyter Notebook Session\nSelect Interactive Apps\nSelect Jupyter Notebook from the list of apps\nSelect desired compute and partition settings.\nSelect Launch\nOnce it starts, select Connect to Jupyter\nIn the Jupyter Notebook Launcher, select micromamba env-name from the Notebook or Console section. It can also be selected as the kernel when choosing the kernel for a new notebook or console.\n\nAnd that’s it!!! You’re connected to the HPC Cluster 😃😃"
  }
]